{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0cc9a631",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Shaping the Transnational Public Sphere in the Chinese press (revisions)\"\n",
    "author: \n",
    "date: \"`r lubridate::today()`\"\n",
    "tags: [bilingual, press, structural topic modeling, transnational]  \n",
    "abstract: |\n",
    "  This document supplements the previous documentation destined to accompany the paper titled \"Shaping the Transnational Public Sphere in Republican China: Discourses and Practices of the Rotary Club in the Shanghai press (1919-1949)\" submitted to the *Journal of Digital History*. This second document provides the code for time-based topic modeling and named entity extraction (part 2), mapping locations (part 3), and semantic analysis (part 4) in the Chinese-language newspaper *Shenbao*. A similar document is devoted specifically to English-language periodicals. \n",
    "  \n",
    "   <style>\n",
    "    body {\n",
    "    text-align: justify}\n",
    "  </style>\n",
    "    \n",
    "output: \n",
    "  html_document:\n",
    "    toc: true\n",
    "    toc_float: \n",
    "      collapsed: false\n",
    "      smooth_scroll: false\n",
    "    toc_depth: 2\n",
    "    number_sections: false\n",
    "    code_folding: show # hide\n",
    "    fig_caption: true\n",
    "    df_print: paged\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c5f8c",
   "metadata": {
    "eval": false,
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "\n",
    "library(tidyverse)\n",
    "library(histtext)\n",
    "library(quanteda)\n",
    "library(tidytext)\n",
    "library(widyr)\n",
    "library(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c72e2c",
   "metadata": {},
   "source": [
    "# Populating the public sphere (Part 2)\n",
    "\n",
    "## Filtering topics (Step 1)\n",
    "\n",
    "Select the documents that contain at least 0.02% of topic 2, 0.01% of topic 3, and 0.02% of topic 10 in the 10-topic model (368 documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bf952",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "library(tidyverse)\n",
    "\n",
    "doc2 <- topicprop10 %>% filter(Topic2 > 0.02) # 282\n",
    "doc3 <- topicprop10 %>% filter(Topic3 > 0.01)  # 340\n",
    "doc10 <- topicprop10 %>% filter(Topic10 > 0.02)  # 250\n",
    "\n",
    "# bind rows and remove duplicates \n",
    "\n",
    "docall <- bind_rows(doc2, doc3, doc10)\n",
    "docall <- unique(docall) # 368 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47eeb2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Refining text units (Step 2) \n",
    "\n",
    "Retrieve the full text of documents from the *Shenbao* corpus (pre-tokenized version) using [histtext](https://bookdown.enpchina.eu/HistText_Book/query-functions.html#full-text-retrieval) and compute the length of articles (number of tokens) to filter articles of sufficient length (those that contain at least 10 tokens). 345 articles remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9a6fa",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(histtext)\n",
    "library(quanteda)\n",
    "\n",
    "docall_ft <- get_documents(docall, \"shunpao-tok\")\n",
    "docall_ft <- docall_ft %>% mutate(length = ntoken(tokenized))\n",
    "\n",
    "# filter out texts with 10 tokens or less\n",
    "\n",
    "docall_ft <- docall_ft %>% filter(length >10) # 345 articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d8d6d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "The revised Chinese corpus contains a total of 345 articles. Next, we export the dataset to extract the relevant news items from mixed-content articles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60089cf0",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "write.csv(docall_ft, \"docall_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1be4a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "We re-import the dataset of properly segmented articles, which will be used for topic modeling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87e8da",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(readr)\n",
    "revised_zh <- read_delim(\"revised_zh.csv\", \n",
    "                         delim = \";\", escape_double = FALSE, trim_ws = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf529160",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Next, we create a customized list of stopwords to remove to improve the results of topic modeling. \n",
    "\n",
    "First, we retrieve the individual tokens using [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html). Additionally, we compute their length and their frequency to select the tokens that contains at least 2 characters and appear more than 10 times in the corpus. In the final line, we export this preliminary list for manual refinement in Excel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8325b",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "\n",
    "token <- revised_zh %>% select(DocId, tokenized)\n",
    "\n",
    "token <- token %>% \n",
    "  unnest_tokens(output = token, input = tokenized) \n",
    "\n",
    "token_count <- token %>% group_by(token) %>% count() %>% mutate(length = nchar(token))\n",
    "\n",
    "stop_word <- token_count %>% filter(length >1) %>% filter(n >10)\n",
    "\n",
    "write.csv(stop_word, \"stop_word.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b0a7f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Finally, we re-import the manually refined list and we transform it into a vector to be used when building topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245ddf8",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# import selected stop words\n",
    "library(readr)\n",
    "stop <- read_csv(\"stop.csv\")\n",
    "\n",
    "# create vector \n",
    "stopvec <- as.vector(stop)\n",
    "stopvec<-unlist(stopvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c2276",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Time-based topic modelling (Step 3)\n",
    "\n",
    "Define time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654fe20",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "revised_eng %>% group_by(period) %>% count()\n",
    "\n",
    "p1 <- revised_zh %>% filter(period == \"1919-1929\") # 62 articles\n",
    "p2 <- revised_zh %>% filter(period == \"1930-1937\") # 141 articles\n",
    "p3 <- revised_zh %>% filter(period == \"1938-1949\") # 142 articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7497ddb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"> \n",
    "In the following, we focus on period 1. The same method was applied to the two later periods. To avoid repetition, we do not replicate the code. </div>\n",
    "\n",
    "Build a 5-topic model using [stm](https://www.structuraltopicmodel.com/):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c73532",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# select metadata\n",
    "\n",
    "meta <- p1 %>% transmute(DocId, Title, Date, year)  \n",
    "meta <- as.data.frame(meta)\n",
    "\n",
    "# create stm corpus object \n",
    "\n",
    "corpus <- stm::textProcessor(p1$tokenized,\n",
    "                             metadata = meta, \n",
    "                             stem = FALSE, \n",
    "                             wordLengths = c(2, Inf), \n",
    "                             verbose = FALSE, \n",
    "                             customstopwords = stopvec) \n",
    "\n",
    "stm::plotRemoved(corpus$documents, lower.thresh = c(0,10, by=5)) \n",
    "\n",
    "out <- stm::prepDocuments(corpus$documents, \n",
    "                          corpus$vocab, \n",
    "                          corpus$meta) \n",
    "\n",
    "# build the 5-topic model \n",
    "\n",
    "# 5-topic model\n",
    "mod.5 <- stm::stm(out$documents, \n",
    "                  out$vocab, K=5, \n",
    "                  prevalence =~ year, \n",
    "                  data=out$meta, verbose = FALSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5fe5e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Estimate the effect of time (year) on topic prevalence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84e1d1",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "year5 <- stm::estimateEffect(1:5 ~ year, mod.5, meta=out$meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc856e9f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Explore the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fa7a4",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot.STM(mod.5,\"summary\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6e0ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "\n",
    "## Topical clusters\n",
    "\n",
    "This section explains how to cluster documents based on their topic proportions using Principal Component Analysis (PCA) and Hierarchical Clustering (HCPC).\n",
    "\n",
    "Extract topic proportions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20dec0",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "topicprop5<-make.dt(mod.5, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da87c0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Prepare the data for PCA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d401114",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pca5 <- topicprop5 %>% select(DocId, Topic1, Topic2, Topic3, Topic4, Topic5)\n",
    "pca5 <- pca5 %>% column_to_rownames(\"DocId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b702742",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Load the [FactoMineR](http://factominer.free.fr/) package and run PCA and HCPC functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43777675",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "library(FactoMineR)\n",
    "\n",
    "res.PCA<-PCA(pca5,graph=FALSE)\n",
    "res.HCPC<-HCPC(res.PCA,nb.clust=5,consol=FALSE,graph=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f57c46",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Optionally, plot the results of PCA and HCPC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73401e77",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot.PCA(res.PCA,choix='var',title=\"PCA graph of variables (topic proportions)\")\n",
    "plot.PCA(res.PCA,title=\"PCA graph of individuals (documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69252a6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Extract and relabel topical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47783081",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# extract clusters \n",
    "pca_clusters <- as.data.frame(res.HCPC$data.clust)\n",
    "pca_clusters <- rownames_to_column(pca_clusters, \"DocId\") %>% select(DocId, clust) %>% rename(clust5 = clust)\n",
    "# add topic labels\n",
    "pca_clusters <- pca_clusters %>% mutate (label5 = fct_recode(clust5, \"International peace\" = \"1\", \n",
    "                                            \"Transpacific networks\" = \"2\", \n",
    "                                            \"Sino-US exchanges\" = \"3\", \n",
    "                                            \"Youth/Health\" = \"4\", \n",
    "                                            \"Education\" = \"5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91f89a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Join clusters with other metadata: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb40906",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "pca_clusters_meta <- left_join(pca_clusters, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d549f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Retrieving actors (Step 4)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"> \n",
    "In the following, we focus on period 1. The same method was applied to the two later periods. To avoid repetition, we do not replicate the code. </div>\n",
    "\n",
    "Extract named entities using histtext [ner_on_df](https://bookdown.enpchina.eu/HistText_Book/named-entity-recognition-ner.html#ner-on-external-documents) function. We applied different models depending on the category of entities. For persons, we applied the model specifically designed for retrieving persons' names, taking into account inserted titles, incomplete names, and co-references. For organizations and locations, we applied the generic spaCy model finely tuned for historical texts in Chinese. More information on these models are available in the [HistText Manual](https://bookdown.enpchina.eu/HistText_Book/named-entity-recognition-ner.html). For more technical details, please refer to Baptiste Blouin's research papers ([Blouin and Magistry, 2020](https://aclanthology.org/2020.paclic-1.1/); [Blouin et al., 2021](https://aclanthology.org/2021.nlp4dh-1.18.pdf)). \n",
    "\n",
    "Extract names of persons: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5297051",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(histtext)\n",
    "p1_pers <- ner_on_df(p1, \"Text\", id_column=\"DocId\", model = \"trftc_person_4class:zh:ner\") # 314 names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030393a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Extract other entities and select the entities of interest (organizations and locations): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3163a",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_ner <- ner_on_df(p1, \"Text\", id_column=\"DocId\", model = \"spacy:zh:ner\")\n",
    "p1_org <- p1_ner %>% filter(Type == \"ORG\") # 352 ORGANIZATIONS\n",
    "p1_loc <- p1_ner %>% filter(Type %in% c(\"LOC\", \"GPE\")) # 229 LOC/GPE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b4f0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Persons \n",
    "\n",
    "The returns a list of names organized into four categories: full names (full), co-references (ref), incomplete names (incomplete), and names with titles (title). For this research, we chose to focus on full names and we compute their frequency to have a preliminary list of the most prominent persons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2f354",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_pers_full <- p1_pers %>% filter(Type == \"Full\") %>% distinct(DocId, Text) \n",
    "\n",
    "p1_pers_full <- p1_pers_full %>% mutate(length = nchar(Text))\n",
    "p1_pers_full <- p1_pers_full %>% group_by(Text) %>% add_tally()\n",
    "p1_top_pers <- p1_pers_full %>% distinct(Text, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118fab9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Build a two-mode network linking persons and the documents in which they are mentioned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554aed4b",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create edge list from list of documents and persons\n",
    "\n",
    "edge <- p1_pers_full\n",
    "\n",
    "# create node list \n",
    "\n",
    "pers_node <- edge %>% select(FullName) %>% rename(Name = FullName) %>% mutate(Type = \"PERS\") %>% unique()\n",
    "doc_node <- edge %>% select(DocId) %>% rename(Name = DocId) %>% mutate(Type = \"DOC\") %>% unique()\n",
    "node <- bind_rows(pers_node, doc_node)\n",
    "\n",
    "# transform edge list into network with igraph\n",
    "\n",
    "library(igraph)\n",
    "\n",
    "e.list <- edge\n",
    "v.attr <- node\n",
    "\n",
    "G <- graph.data.frame(e.list, vertices=v.attr, directed=FALSE)\n",
    "\n",
    "# index the color and shape to the type of node\n",
    "\n",
    "bipartite.mapping(G)\n",
    "\n",
    "V(G)$type <- bipartite_mapping(G)$type\n",
    "V(G)$color <- ifelse(V(G)$type, \"red\", \"orange\")\n",
    "V(G)$shape <- ifelse(V(G)$type, \"square\", \"circle\")\n",
    "E(G)$color <- \"lightgray\"\n",
    "\n",
    "\n",
    "l <- layout.fruchterman.reingold(G)\n",
    "\n",
    "# plot the network\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     vertex.size=5, \n",
    "     vertex.label.cex=0.3,\n",
    "     vertex.label.color=\"black\", \n",
    "     vertex.label.family=\"Arial\")\n",
    "\n",
    "# remove labels to enhance legibility\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     vertex.size=5, \n",
    "     vertex.label.cex=0.3,\n",
    "     vertex.label = NA, \n",
    "     vertex.label.color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c0941",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Organizations \n",
    "\n",
    "Data cleaning of organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4086e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# atomize list of names \n",
    "\n",
    "p1_org <- p1_org %>% separate_rows(Text, sep = \"·\", convert = FALSE) # we now have 395 organizations\n",
    "\n",
    "# count characters \n",
    "\n",
    "p1_org <- p1_org %>% mutate(length = nchar(Text))\n",
    "\n",
    "# extract the 2 last characters to build a preliminary categorization of the organizations\n",
    "\n",
    "p1_org <- p1_org %>% mutate(suf2 = str_sub(Text,-2,-1))\n",
    "\n",
    "# export for manual refinement in Excel \n",
    "\n",
    "write.csv(p1_org_zh, \"p1_org_zh.csv\")\n",
    "\n",
    "# reimport clean data\n",
    "\n",
    "library(readr)\n",
    "p1_org_clean <- read_delim(\"p1_org_clean.csv\", \n",
    "                              delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# recount length\n",
    "\n",
    "p1_org_clean <- p1_org_clean %>% mutate(length = nchar(Text_clean))\n",
    "\n",
    "# extract last characters again to classify organizations \n",
    "\n",
    "p1_org_clean <- p1_org_clean %>% mutate(class = str_sub(Text_clean,-2,-1)) \n",
    "\n",
    "# identify characteristics of particular interest (Shanghai-based organizations, Rotary, YMCA, and business enterprises)  \n",
    "p1_org_clean <- p1_org_clean %>%  mutate(local = str_extract(Text, \"上海\")) %>% \n",
    "  mutate(ymca = str_extract(Text, \"青年\")) %>% \n",
    "  mutate(rotary = str_extract(Text, \"扶輪\"))  %>% \n",
    "  mutate(company = str_extract(Text, \"公司\")) \n",
    "\n",
    "# select unique pairs\n",
    "\n",
    "p1_org_unique <- p1_org_clean %>% distinct(DocId, Text_clean, class, local, rotary, ymca) # 262 unique pairs\n",
    "\n",
    "# rank organizations based on their frequency\n",
    "\n",
    "p1_org_count<- p1_org_unique %>% distinct(DocId, Text_clean) %>% group_by(Text_clean) %>% count()\n",
    "\n",
    "# count by categories/sectors\n",
    "\n",
    "p1_org_unique %>% distinct(Text_clean, class) %>% group_by(class) %>% count(sort = TRUE)\n",
    "\n",
    "# export edge list and node list \n",
    "\n",
    "p1_edge_org <- p1_org_unique %>% select(DocId, Text_clean)\n",
    "p1_node_org  <- p1_org_unique %>% distinct(Text_clean, class)\n",
    "\n",
    "write.csv(p1_edge_org, \"p1zh_pers_edge.csv\")\n",
    "write.csv(p1_node_org, \"p1zh_org_node.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf1ba7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Locations \n",
    "\n",
    "Atomize list, count length and frequency, and export for further processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e70ae",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_loc <- p1_loc %>% separate_rows(Text, sep = \"·\", convert = FALSE) %>% \n",
    "  mutate(length = nchar(Text)) %>% \n",
    "  group_by(Text) %>% \n",
    "  add_tally()\n",
    "\n",
    "write.csv(p1_p1_locloc_zh, \"p1_loc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b4273",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Mapping the Public Sphere (Part 3)\n",
    "\n",
    "## Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ceb14",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load locations data \n",
    "\n",
    "loc_map <- read_delim(\"maps/loc_map.csv\", \n",
    "                      delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# select countries \n",
    "\n",
    "loc_map_country  <-  loc_map %>% mutate(Country = Place) %>% mutate(Prov_Py = Province) %>% \n",
    "  filter(Type == \"Country\") %>% mutate(Prov_Py = Province) %>% \n",
    "  select(Language, Period, Type, Country, Count)\n",
    "\n",
    "# load packages\n",
    "\n",
    "install.packages(\"tmap\", repos = c(\"https://r-tmap.r-universe.dev\",\n",
    "                                   \"https://cloud.r-project.org\"))\n",
    "library(tmap)    # for static and interactive maps\n",
    "library(leaflet) # for interactive maps\n",
    "library(sf)\n",
    "library(maps)\n",
    "\n",
    "# load country data\n",
    "\n",
    "world_name <- world %>% as.data.frame()\n",
    "world_name <- world_name %>% rename(Country = name_long)\n",
    "\n",
    "# join with my list of countries\n",
    "\n",
    "loc_map_country <- inner_join(loc_map_country, world_name)\n",
    "loc_map_country_zh <- loc_map_country %>% filter(Language == \"Chinese\")\n",
    "\n",
    "# compute mean frequencies across period\n",
    "\n",
    "loc_map_country_zh_mean <- loc_map_country_zh %>% \n",
    "  group_by(Country, iso_a2, geom) %>% \n",
    "  summarise(mean = round(mean(Count), 0)) %>% ungroup()\n",
    "\n",
    "# convert dataframe into shapefile \n",
    "\n",
    "loc_map_country_zh_sf <- st_as_sf(loc_map_country_zh_mean, sf_column_name = \"geom\", crs = \"WGS84\")\n",
    "\n",
    "# Create a color palette for the map using bins and quantiles \n",
    "\n",
    "mypalette_zh <- colorBin( palette=\"YlOrRd\", domain=loc_map_country_zh_mean$mean, na.color=\"transparent\", bins = 5)\n",
    "mypalette_zh2 <- colorQuantile(palette=\"YlOrRd\", domain=loc_map_country_zh_mean$mean, na.color=\"transparent\")\n",
    "\n",
    "# create choropleth maps\n",
    "\n",
    "leaflet(loc_map_country_zh_sf) %>% \n",
    "  addTiles()  %>% \n",
    "  setView( lat=10, lng=0 , zoom=2) %>%\n",
    "  addPolygons( stroke = FALSE, fillOpacity = 0.9, smoothFactor = 0.5, color = ~colorBin(\"YlOrRd\", mean)(mean) ) %>%\n",
    "  addLegend( pal=mypalette_zh, values=~mean, opacity=0.9, title = \"Mean frequency\", position = \"bottomleft\" )\n",
    "\n",
    "\n",
    "leaflet(loc_map_country_zh_sf) %>% \n",
    "  addTiles()  %>% \n",
    "  setView( lat=10, lng=0 , zoom=2) %>%\n",
    "  addPolygons( stroke = FALSE, fillOpacity = 0.9, smoothFactor = 0.5, color = ~colorQuantile(\"YlOrRd\", mean)(mean) ) %>%\n",
    "  addLegend( pal=mypalette_zh2, values=~mean, opacity=0.9, title = \"Mean frequency\", position = \"bottomleft\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0d782",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3e0e2",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# create city data\n",
    "\n",
    "loc_map_zh_to_join <-  loc_map %>% mutate(Name = Place) %>% mutate(Prov_Py = Province) %>% filter(Country == \"China\")\n",
    "\n",
    "# Chinese cities/provinces\n",
    "\n",
    "MCGD_Data2023.06.21 <- read.csv(\"~/publicsphere/maps/MCGD_Data2023-06-21.csv\")\n",
    "mcgd <- MCGD_Data2023.06.21\n",
    "\n",
    "# join with Chinese coordinates \n",
    "\n",
    "zh_city <- inner_join(loc_map_zh_to_join, mcgd)\n",
    "\n",
    "missing <- setdiff(loc_map_zh_to_join$Name, zh_city$Name)\n",
    "missing <- as.data.frame(missing)\n",
    "\n",
    "write.csv(zh_city, \"zh_city.csv\")\n",
    "\n",
    "# reload clean data\n",
    "\n",
    "zh_city2 <- read_delim(\"maps/zh_city2.csv\", \n",
    "                       delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# filter non Chinese cities\n",
    "\n",
    "loc_map_cities <-  loc_map %>% mutate(Name = Place) %>% filter(Type == \"City\") %>% filter(!Country == \"China\")\n",
    "\n",
    "# join with geocoordinates\n",
    "\n",
    "library(maps)\n",
    "data(world.cities)\n",
    "\n",
    "world_cities <- world.cities %>% as.data.frame()\n",
    "world_cities2 <- world_cities %>% mutate(name = str_remove_all(name, \"'\"))\n",
    "world_cities2 <- world_cities2 %>% select(Name, country.etc, lat, long)\n",
    "\n",
    "loc_map_cities_latlong <- inner_join(loc_map_cities, world_cities2)\n",
    "loc_map_cities_latlong <- left_join(loc_map_cities, world_cities2)\n",
    "\n",
    "# find duplicates \n",
    "\n",
    "pb <- loc_map_cities_latlong %>% group_by(Name, country.etc) %>% count() %>% filter(n>1)\n",
    "\n",
    "# reload cleaned cities \n",
    "\n",
    "cities_clean <- read_delim(\"cities_clean.csv\", \n",
    "                           delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "cities_to_join <- cities_clean %>% select(Language, Period, Country, Name, Count, lat, long)\n",
    "\n",
    "zh_city2_to_join <- zh_city2 %>% select(Language, Period, Country, Name, Count, LAT, LONG) %>% rename(lat = LAT, long = LONG)  \n",
    "\n",
    "all_cities <- bind_rows(cities_to_join, zh_city2_to_join)\n",
    "\n",
    "all_cities <- all_cities %>% mutate(City = str_replace(Name, \"Jiangsu\", \"Shanghai\"))\n",
    "\n",
    "# filter by language \n",
    "\n",
    "all_cities_zh <- all_cities %>% filter(Language == \"Chinese\")\n",
    "\n",
    "# compute mean \n",
    "\n",
    "loc_map_city_zh_mean <- all_cities_zh %>% \n",
    "  group_by(Name, lat, long) %>% \n",
    "  summarise(mean = round(mean(Count), 0)) %>% ungroup()\n",
    "\n",
    "# map with leaflet\n",
    "\n",
    "loc_map_city_zh_mean %>%\n",
    "  leaflet() %>%\n",
    "  addTiles() %>%\n",
    "  addCircleMarkers( radius = ~log(mean)*3,\n",
    "                    label = ~Name,\n",
    "                    color = \"white\",\n",
    "                    weight = 2,\n",
    "                    opacity = 0.6,\n",
    "                    fill = TRUE,\n",
    "                    fillColor = \"red\",\n",
    "                    fillOpacity = 0.9,\n",
    "                    stroke = TRUE,\n",
    "                    popup = ~paste( \"City:\", Name ,\n",
    "                                    \"\",\n",
    "                                    \"Mean frequency:\", mean))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1937e1f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Articulating the Public Sphere (Part 4)\n",
    "\n",
    "## 國-based terms\n",
    "\n",
    "### Concordance \n",
    "\n",
    "Retrieve 國-based terms using concordance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097ffc6",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load packages\n",
    "\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(widyr)\n",
    "library(tidytext)\n",
    "\n",
    "library(igraph)\n",
    "library(tidygraph)\n",
    "library(ggraph)\n",
    "\n",
    "# select relevant variables (Text and DocId) from the initial corpora \n",
    "\n",
    "guo <- revised_zh %>% select(DocId, tokenized)\n",
    "\n",
    "# retrieve 國 in context\n",
    "\n",
    "guo_conc<- histtext::search_concordance_on_df(sample, \"國\", id_column = \"DocId\", \n",
    "                                               context_size = 20, \n",
    "                                               case_sensitive = FALSE)\n",
    "\n",
    "# select first characters in after and join match + after \n",
    "\n",
    "guo2 <- guo_conc %>% select(DocId, Before, Match, After)%>% \n",
    "  mutate(Before = str_replace(Before, \"、\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"·\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"·\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"）\", \"\")) %>%\n",
    "  mutate(Before = str_replace(Before, \"，\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"、\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"：\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"「\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"〕\", \"\"))%>% \n",
    "  mutate(After = str_replace(After, \"」\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"，\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"、\", \"\"))%>% \n",
    "  mutate(After = str_replace(After, \"）\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"·\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"」\", \"\")) %>% \n",
    "  mutate(tok1 =  str_sub(Before, -1,-1)) %>% \n",
    "  mutate(tok3 =  str_sub(After, 1,1)) %>% \n",
    "  rename(tok2 = Match) %>%\n",
    "  mutate(bigram = paste0(tok1, tok2))%>%\n",
    "  mutate(bigram2 = paste0(tok2, tok3))%>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"·\", \"\")) %>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"　\", \"\")) %>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"。\", \"\"))%>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"（\", \"\"))%>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"「\",\"\"))\n",
    "\n",
    "# compile all terms \n",
    "\n",
    "guo_bigram1 <- guo2 %>% select(-c(bigram2))\n",
    "guo_bigram2 <- guo2 %>% select(-c(bigram)) %>% rename(bigram = bigram2)\n",
    "guo_all <- bind_rows(guo_bigram1, guo_bigram2)\n",
    "\n",
    "# count frequencies \n",
    "\n",
    "guo_count <- guo_all %>% group_by(bigram) %>% count() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21596ecd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### TF-IDF by period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f527234",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# join with metadata \n",
    "\n",
    "guo_period <- left_join(guo_all, meta)\n",
    "\n",
    "# compute tf-idf by period \n",
    "\n",
    "guo_tf_idf_period <- guo_period %>%\n",
    "  count(period, bigram) %>%\n",
    "  bind_tf_idf(bigram, period, n) %>%\n",
    "  arrange(desc(tf_idf))\n",
    "\n",
    "# Visualize \n",
    "\n",
    "guo_tf_idf_period %>%\n",
    "  group_by(period) %>%\n",
    "  top_n(10, tf_idf) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(bigram = reorder(bigram, tf_idf)) %>%\n",
    "  ggplot(aes(tf_idf, bigram, fill = period)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ period, scales = \"free\", ncol=3) +\n",
    "  labs(x = \"tf-idf\", y = \"term\", \n",
    "       title = \"Highest tf-idf terms associated with \\\"國\\\" in the rotary corpus\", \n",
    "       subtitle = \"tf-idf by period\", \n",
    "       caption = \"Based on *Shenbao 申報*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50099cfd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Collocates\n",
    "\n",
    "Prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935d007",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# extract and count tokens\n",
    "\n",
    "rotary_tidy_token <- revised_zh %>% \n",
    "  unnest_tokens(output = token, input = tokenized) \n",
    "\n",
    "# compute pairwise count \n",
    "\n",
    "token_pairs <- rotary_tidy_token %>%\n",
    "  pairwise_count(token, DocId, sort = TRUE)\n",
    "\n",
    "# create list of stop words \n",
    "\n",
    "rotary_tokens <- rotary_tidy_token %>% mutate(length = nchar(token)) %>% group_by(token) %>% add_tally()\n",
    "rotary_tokens_filtered <- rotary_tokens %>% filter(length >1) %>% filter(n >1)\n",
    "rotary_tokens_count <- rotary_tokens_filtered  %>% distinct(token, length, n)\n",
    "rotary_tokens_filtered_simple <- rotary_tokens_filtered %>% select(DocId, token)\n",
    "\n",
    "rotary_stopwords <- rotary_tokens %>% filter(length <2)\n",
    "rotary_stopwords <- rotary_stopwords %>% select(token)\n",
    "rotary_stopwords <- rotary_stopwords %>% unique()\n",
    "\n",
    "# remove stop words \n",
    "\n",
    "token_pairs_filtered <- token_pairs %>% rename(token = item1) %>% \n",
    "  anti_join(rotary_stopwords) %>% rename(item1 = token) %>% \n",
    "  rename(token = item2) %>% \n",
    "  anti_join(rotary_stopwords) %>% rename(item2 = token) %>% \n",
    "  filter(!item1 == \"扶輪社\") %>% \n",
    "  filter(!item2  == \"扶輪社\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b73aaa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Focus on 國-based terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2add0",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# create list of terms of interest \n",
    "\n",
    "guolist <- guo_count %>% filter(bigram %in% c(\"國際\", \"各國\", \"我國\", \"全國\", \"萬國\", \"國家\", \"外國\", \"國民\", \"國人\", \"國貨\", \"國内\", \"國外\", \"回國\", \"進國\", \"民國\"))\n",
    "\n",
    "# select these terms in the list of collocates\n",
    "\n",
    "guo1 <- token_pairs_filtered %>%\n",
    "  filter(item1 == guolist$bigram)\n",
    "guo2 <- token_pairs_filtered %>%\n",
    "  filter(item2 == guolist$bigram)\n",
    "guo_cooc <- bind_rows(guo1, guo2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411a256",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Select most important pairs to include in the network of collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41719f9f",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "guo_cooc_filtered <- guo_cooc %>% filter(n > 2)\n",
    "guo_cooc_node1 <- guo_cooc_filtered %>% select(item1) %>% unique() %>% rename(token = item1)\n",
    "guo_cooc_node2 <- guo_cooc_filtered %>% select(item2) %>% unique() %>% rename(token = item2)\n",
    "guo_cooc_node_filtered <- bind_rows(guo_cooc_node1, guo_cooc_node2)\n",
    "guo_cooc_node_filtered <- guo_cooc_node_filtered %>% unique()\n",
    "guo_cooc_node_filtered <- inner_join(guo_cooc_node_filtered, rotary_tokens_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96806a0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Build a co-ocurrence network with [igraph](https://r.igraph.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76f438",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(igraph)\n",
    "\n",
    "e.list <- guo_cooc_filtered\n",
    "v.attr <- guo_cooc_node_filtered\n",
    "\n",
    "G <- graph.data.frame(e.list, vertices=v.attr, directed=FALSE)\n",
    "\n",
    "v.size <- V(G)$n\n",
    "v.label <- V(G)$name\n",
    "E(G)$weight <- E(G)$n\n",
    "eigenCent <- evcent(G)$vector\n",
    "\n",
    "head(eigenCent, 20)\n",
    "\n",
    "plot(sort(eigenCent, decreasing=TRUE), type=\"l\")\n",
    "\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "V(G)$color <- colorVals\n",
    "\n",
    "l <- layout.fruchterman.reingold(G)\n",
    "\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     edge.width=E(G)$weight*0.01,\n",
    "     vertex.size=v.size*0.01, \n",
    "     vertex.label=v.label,\n",
    "     vertex.label.cex=v.size*0.001,\n",
    "     vertex.label.color=\"black\", \n",
    "     vertex.label.dist=0.2,\n",
    "     vertex.label.family=\"Arial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b91fff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Refine network visualization with [tidygraph](https://tidygraph.data-imaginist.com/) and [gggraph](https://github.com/thomasp85/ggraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6df091",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "library(tidygraph)\n",
    "library(ggraph)\n",
    "\n",
    "tg <- tidygraph::as_tbl_graph(G) %>% activate(nodes) %>% mutate(label=name)\n",
    "\n",
    "v.size <- V(tg)$n\n",
    "E(tg)$weight <- E(tg)$n\n",
    "eigenCent <- evcent(tg)$vector\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "\n",
    "\n",
    "tg %>%\n",
    "  ggraph(layout=\"kk\") +\n",
    "  geom_edge_link(alpha = .25, colour='white', aes(width = weight)) +\n",
    "  geom_node_point(size=log(v.size)*2, color=colorVals) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \"lines\"), size=log(v.size), colour=\"white\") +\n",
    "  theme_graph(background = 'grey20')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c609ecf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 公-based terms\n",
    "\n",
    "### Concordance \n",
    "\n",
    "Retrieve 公-based terms using concordance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc2695",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load packages\n",
    "\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(widyr)\n",
    "library(tidytext)\n",
    "\n",
    "library(igraph)\n",
    "library(tidygraph)\n",
    "library(ggraph)\n",
    "\n",
    "# select relevant variables (Text and DocId) from the initial corpora \n",
    "\n",
    "gong <- revised_zh %>% select(DocId, tokenized)\n",
    "\n",
    "# retrieve 公 in context\n",
    "\n",
    "gong_conc<- histtext::search_concordance_on_df(sample, \"公\", id_column = \"DocId\", \n",
    "                                               context_size = 20, \n",
    "                                               case_sensitive = FALSE)\n",
    "\n",
    "# select first characters in after and join match + after \n",
    "\n",
    "gong2 <- gong_conc %>% select(DocId, Before, Match, After)%>% \n",
    "  mutate(Before = str_replace(Before, \"、\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"·\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"·\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"）\", \"\")) %>%\n",
    "  mutate(Before = str_replace(Before, \"，\", \"\")) %>% \n",
    "  mutate(Before = str_replace(Before, \"、\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"：\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"「\", \"\"))%>% \n",
    "  mutate(Before = str_replace(Before, \"〕\", \"\"))%>% \n",
    "  mutate(After = str_replace(After, \"」\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"，\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"、\", \"\"))%>% \n",
    "  mutate(After = str_replace(After, \"）\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"·\", \"\")) %>% \n",
    "  mutate(After = str_replace(After, \"」\", \"\")) %>% \n",
    "  mutate(tok1 =  str_sub(Before, -1,-1)) %>% \n",
    "  mutate(tok3 =  str_sub(After, 1,1)) %>% \n",
    "  rename(tok2 = Match) %>%\n",
    "  mutate(bigram = paste0(tok1, tok2))%>%\n",
    "  mutate(bigram2 = paste0(tok2, tok3))%>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"·\", \"\")) %>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"　\", \"\")) %>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"。\", \"\"))%>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"（\", \"\"))%>% \n",
    "  mutate(bigram2 = str_replace_all(bigram2, \"「\",\"\"))\n",
    "\n",
    "# compile all terms \n",
    "\n",
    "gong_bigram1 <- gong2 %>% select(-c(bigram2))\n",
    "gong_bigram2 <- gong2 %>% select(-c(bigram)) %>% rename(bigram = bigram2)\n",
    "gong_all <- bind_rows(gong_bigram1, gong_bigram2)\n",
    "\n",
    "# count frequencies \n",
    "\n",
    "gong_count <- gong_all %>% group_by(bigram) %>% count() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebf7ae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### TF-IDF by period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b5448",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# join with metadata \n",
    "\n",
    "gong_period <- left_join(gong_all, meta)\n",
    "\n",
    "# compute tf-idf by period \n",
    "\n",
    "gong_tf_idf_period <- gong_period %>%\n",
    "  count(period, bigram) %>%\n",
    "  bind_tf_idf(bigram, period, n) %>%\n",
    "  arrange(desc(tf_idf))\n",
    "\n",
    "# Visualize \n",
    "\n",
    "gong_tf_idf_period %>%\n",
    "  group_by(period) %>%\n",
    "  top_n(10, tf_idf) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(bigram = reorder(bigram, tf_idf)) %>%\n",
    "  ggplot(aes(tf_idf, bigram, fill = period)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ period, scales = \"free\", ncol=3) +\n",
    "  labs(x = \"tf-idf\", y = \"term\", \n",
    "       title = \"Highest tf-idf terms associated with \\\"公\\\" in the rotary corpus\", \n",
    "       subtitle = \"tf-idf by period\", \n",
    "       caption = \"Based on *Shenbao 申報*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e71afc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Collocates\n",
    "\n",
    "Focus on 公-based terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55f313",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# create list of terms of interest (remove 公司)\n",
    "\n",
    "gonglist <- gong_count %>% filter(!bigram == \"公司\")\n",
    "\n",
    "# select these terms in the list of collocates\n",
    "\n",
    "gong1 <- token_pairs_filtered %>%\n",
    "  filter(item1 == gonglist$bigram)\n",
    "gong2 <- token_pairs_filtered %>%\n",
    "  filter(item2 == gonglist$bigram)\n",
    "gong_cooc <- bind_rows(gong1, gong2)\n",
    "\n",
    "gong1\n",
    "gong_cooc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb038abf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Select most important pairs to include in the network of collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c98e98",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "gong_cooc_filtered <- gong_cooc %>% filter(n > 1)\n",
    "gong_cooc_node1 <- gong_cooc_filtered %>% select(item1) %>% unique() %>% rename(token = item1)\n",
    "gong_cooc_node2 <- gong_cooc_filtered %>% select(item2) %>% unique() %>% rename(token = item2)\n",
    "gong_cooc_node_filtered <- bind_rows(gong_cooc_node1, gong_cooc_node2)\n",
    "gong_cooc_node_filtered <- gong_cooc_node_filtered %>% unique()\n",
    "gong_cooc_node_filtered <- inner_join(gong_cooc_node_filtered, rotary_tokens_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceaa85",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Build a co-ocurrence network with [igraph](https://r.igraph.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a2451",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(igraph)\n",
    "\n",
    "e.list <- gong_cooc_filtered\n",
    "v.attr <- gong_cooc_node_filtered\n",
    "\n",
    "G <- graph.data.frame(e.list, vertices=v.attr, directed=FALSE)\n",
    "\n",
    "v.size <- V(G)$n\n",
    "v.label <- V(G)$name\n",
    "E(G)$weight <- E(G)$n\n",
    "eigenCent <- evcent(G)$vector\n",
    "\n",
    "head(eigenCent, 20)\n",
    "\n",
    "plot(sort(eigenCent, decreasing=TRUE), type=\"l\")\n",
    "\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "V(G)$color <- colorVals\n",
    "\n",
    "l <- layout.fruchterman.reingold(G)\n",
    "\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     edge.width=E(G)$weight*0.01,\n",
    "     vertex.size=v.size*0.01, \n",
    "     vertex.label=v.label,\n",
    "     vertex.label.cex=v.size*0.001,\n",
    "     vertex.label.color=\"black\", \n",
    "     vertex.label.dist=0.2,\n",
    "     vertex.label.family=\"Arial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c59365",
   "metadata": {},
   "source": [
    "<br>\n",
    "Refine network visualization with [tidygraph](https://tidygraph.data-imaginist.com/) and [gggraph](https://github.com/thomasp85/ggraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd440a4",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(tidygraph)\n",
    "library(ggraph)\n",
    "\n",
    "tg <- tidygraph::as_tbl_graph(G) %>% activate(nodes) %>% mutate(label=name)\n",
    "\n",
    "v.size <- V(tg)$n\n",
    "E(tg)$weight <- E(tg)$n\n",
    "eigenCent <- evcent(tg)$vector\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "\n",
    "\n",
    "tg %>%\n",
    "  ggraph(layout=\"kk\") +\n",
    "  geom_edge_link(alpha = .25, colour='white', aes(width = weight)) +\n",
    "  geom_node_point(size=5, color=colorVals) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \"lines\"), size=5, colour=\"white\") +\n",
    "  theme_graph(background = 'grey20')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6f9e0",
   "metadata": {},
   "source": [
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "eval,tags,name,-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
