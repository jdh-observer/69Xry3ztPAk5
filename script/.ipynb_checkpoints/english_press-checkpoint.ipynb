{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afb4c40d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Shaping the Transnational Public Sphere in the English press (revisions)\"\n",
    "author: \n",
    "date: \"`r lubridate::today()`\"\n",
    "tags: [bilingual, press, structural topic modeling, transnational]  \n",
    "abstract: |\n",
    "  This document supplements the previous documentation destined to accompany the paper titled \"Shaping the Transnational Public Sphere in Republican China: Discourses and Practices of the Rotary Club in the Shanghai press (1919-1949)\" submitted to the *Journal of Digital History*. This second document provides the code for time-based topic modeling and named entity extraction (part 2), mapping locations (part 3), and semantic analysis (part 4) in the ProQuest collection of English-language periodicals. A similar document is devoted specifically to the Chinese-language newspaper *Shenbao*.  \n",
    "  \n",
    "   <style>\n",
    "    body {\n",
    "    text-align: justify}\n",
    "  </style>\n",
    "    \n",
    "output: \n",
    "  html_document:\n",
    "    toc: true\n",
    "    toc_float: \n",
    "      collapsed: false\n",
    "      smooth_scroll: false\n",
    "    toc_depth: 2\n",
    "    number_sections: false\n",
    "    code_folding: show # hide\n",
    "    fig_caption: true\n",
    "    df_print: paged\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcbfff",
   "metadata": {
    "eval": false,
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "\n",
    "library(tidyverse)\n",
    "library(histtext)\n",
    "library(quanteda)\n",
    "library(tidytext)\n",
    "library(widyr)\n",
    "library(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047699a5",
   "metadata": {},
   "source": [
    "# Populating the public sphere (Part 2)\n",
    "\n",
    "## Filtering topics (Step 1)\n",
    "\n",
    "Select the documents that contain at least 0.11% of topic 3 in the 5-topic model (1256 documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ceb97",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "doc3 <- topicprop5 %>% filter(Topic3 > 0.11)  # 1256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e67fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Refining text units (Step 2) \n",
    "\n",
    "Focus on \"feature\" articles and remove non-relevant genres of articles (984 documents remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5dc62",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "doc3_filtered <- doc3 %>% filter(!category %in% c(\"Image/Photograph\", \"Obituary\", \"Stock Quote\")) %>% filter(!DocId == \"1420033734\") \n",
    "\n",
    "doc_to_remove <- doc3_filtered %>%\n",
    "  filter(\n",
    "    str_detect(Title, \"No Title|Brevities|BREVITIES|Men and Events|Notes|NOTES\")\n",
    "  ) %>% \n",
    "  filter(!category == (\"Editorial, Commentary\")) %>% \n",
    "  filter(!category == (\"Letter to the Editor, Correspondence\")) %>% \n",
    "  filter(!DocId == \"1371424827\") # 257 brevities to remove \n",
    "\n",
    "doc3_filtered <- doc3_filtered %>% filter(!DocId %in% doc_to_remove$DocId)\n",
    "\n",
    "doc3_cat <- doc3_filtered %>% select(DocId, category) # select relevant metadata \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471cc42",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Retrieve the full text of documents using [histtext](https://bookdown.enpchina.eu/HistText_Book/query-functions.html#full-text-retrieval) (the current dataset contains only document ids), compute the length of articles (number of tokens) with [quanteda](https://quanteda.io/), and join with metadata (category): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998b610",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "library(histtext)\n",
    "library(quanteda)\n",
    "\n",
    "doc3_ft <- get_documents(doc3_filtered, \"proquest\")\n",
    "doc3_ft <- doc3_ft %>% mutate(length = ntoken(Text))\n",
    "doc3_ft <- left_join(doc3_cat, doc3_ft) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618136c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Export the dataset to check manually abnormally long articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84e461",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "write.csv(doc3_ft, \"doc3_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4028e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Reimport shortened articles, remove these articles from the main list, and compile the two datasets to reconstitute the complete corpus with properly segmented news items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06013d",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(readr)\n",
    "doc3_short <- read_delim(\"doc3_short.csv\", \n",
    "                         delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# remove these articles from the main list \n",
    "\n",
    "doc3_filtered <- doc3_ft %>% filter(!DocId %in% doc3_short$DocId)\n",
    "\n",
    "# compile the two datasets using bind rows\n",
    "\n",
    "doc3_filtered$Topic3 <- NULL\n",
    "doc3_short <- doc3_short %>% mutate(DocId = as.character(DocId)) %>% mutate(Date = as.character(Date))\n",
    "\n",
    "revised_eng <- bind_rows(doc3_filtered, doc3_short)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36cf840",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "The revised English corpus contains a total of 954 articles. \n",
    "\n",
    "Next, we create a customized list of stopwords to remove to improve the results of topic modeling. \n",
    "\n",
    "First, we retrieve the tokens and we compute their length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14f90b",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "\n",
    "token <- revised_eng %>% select(DocId, Text)\n",
    "\n",
    "token <- token %>% \n",
    "  unnest_tokens(output = token, input = Text) \n",
    "\n",
    "token_count <- token %>% group_by(token) %>% count() %>% mutate(length = nchar(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53a10b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Next, we create a preliminary list of stopwords by filtering tokens which appeared at least twice and contains at least 5 characters, and by removing non alphabetical characters (digits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261644f",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "stopwords <- token_count %>% filter(length>4) %>% filter(n >1) %>% \n",
    "  filter(!grepl(\"[[:digit:]]\", token)) # remove digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab59df8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "We export the raw list for manual refinement in Excel, we re-import the manually refined list and we transform it into a vector which can be used later for building topic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f87b5",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "stopwords <- read_csv(\"stopwords.csv\")\n",
    "stopvec <- as.vector(stopwords)\n",
    "stopvec<-unlist(stopvec)\n",
    "stopvec<-unlist(stopvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef04e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Time-based topic modelling (Step 3)\n",
    "\n",
    "Define time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1549c",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "revised_eng %>% group_by(period) %>% count()\n",
    "\n",
    "p1 <- revised_eng %>% filter(period == \"1919-1929\") # 184 articles\n",
    "p2 <- revised_eng %>% filter(period == \"1930-1937\") # 687\n",
    "p3 <- revised_eng %>% filter(period == \"1938-1949\") # 83 articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d90f05",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"> \n",
    "In the following, we focus on period 1. The same method was applied to the two later periods. To avoid repetition, we do not replicate the code. </div>\n",
    "\n",
    "Build a 5-topic model using [stm](https://www.structuraltopicmodel.com/):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864c405",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# select metadata\n",
    "\n",
    "meta <- p1 %>%  transmute(DocId, Title, Source, category, Date, Year, length)\n",
    "meta <- as.data.frame(meta)\n",
    "\n",
    "# create stm corpus object \n",
    "\n",
    "corpus <- stm::textProcessor(p1$Text,\n",
    "                             metadata = meta, \n",
    "                             stem = FALSE, \n",
    "                             wordLengths = c(5, Inf), \n",
    "                             verbose = FALSE, \n",
    "                             customstopwords = stopvec)\n",
    "\n",
    "stm::plotRemoved(corpus$documents, lower.thresh = c(0,10, by=5))\n",
    "\n",
    "# chose 5 as the threshold for minimum frequency \n",
    "\n",
    "out <- stm::prepDocuments(corpus$documents, \n",
    "                          corpus$vocab, \n",
    "                          corpus$meta, \n",
    "                          lower.thresh = 5)\n",
    "# 10562 of 11702 terms (17079 of 30220 tokens) are removed due to frequency. \n",
    "# The corpus now has 184 documents, 1140 terms and 13141 tokens.\n",
    "\n",
    "# build the 5-topic model \n",
    "\n",
    "mod.5 <- stm::stm(out$documents, \n",
    "                  out$vocab, K=5, \n",
    "                  data=out$meta, \n",
    "                  prevalence =~ Year + Source + category, \n",
    "                  verbose = FALSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2e1f6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Estimate the effects of time (year) and source (periodical) on topic prevalence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a092833",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "year5 <- stm::estimateEffect(1:5 ~ Year, mod.5, meta=out$meta)\n",
    "source5 <- stm::estimateEffect(1:5 ~ Source, mod.5, meta=out$meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19c57b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Explore the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b3005",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot.STM(mod.5,\"summary\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3c07e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "\n",
    "## Topical clusters\n",
    "\n",
    "This section explains how to cluster documents based on their topic proportions using Principal Component Analysis (PCA) and Hierarchical Clustering (HCPC).\n",
    "\n",
    "Extract topic proportions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b81dc4",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "topicprop5<-make.dt(mod.5, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fd97a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Prepare the data for PCA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033acb9",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pca5 <- topicprop5 %>% select(DocId, Topic1, Topic2, Topic3, Topic4, Topic5)\n",
    "pca5 <- pca5 %>% column_to_rownames(\"DocId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c2990",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Load the [FactoMineR](http://factominer.free.fr/) package and run PCA and HCPC functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d6732",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "library(FactoMineR)\n",
    "\n",
    "res.PCA<-PCA(pca5,graph=FALSE)\n",
    "res.HCPC<-HCPC(res.PCA,nb.clust=5,consol=FALSE,graph=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd703211",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Optionally, plot the results of PCA and HCPC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6d8b4",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot.PCA(res.PCA,choix='var',title=\"PCA graph of variables (topic proportions)\")\n",
    "plot.PCA(res.PCA,title=\"PCA graph of individuals (documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79733933",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Extract and relabel topical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540804f",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pca_clusters <- as.data.frame(res.HCPC$data.clust)\n",
    "pca_clusters <- rownames_to_column(pca_clusters, \"DocId\") %>% select(DocId, clust) %>% rename(clust5 = clust)\n",
    "pca_clusters <- pca_clusters %>% mutate (label5 = fct_recode(clust5, \"American education\" = \"1\", \n",
    "                                                             \"Pacific relations\" = \"2\", \n",
    "                                                             \"Water conservancy and public health\" = \"3\", \n",
    "                                                             \"Labor, industry, and Soviet Russia\" = \"4\", \n",
    "                                                             \"International peace\" = \"5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06f710",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Join clusters with other metadata: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548fe99",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "pca_clusters_meta <- left_join(pca_clusters, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb13a6a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Retrieving actors (Step 4)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\"> \n",
    "In the following, we focus on period 1. The same method was applied to the two later periods. To avoid repetition, we do not replicate the code. </div>\n",
    "\n",
    "Extract named entities using histtext [ner_on_df](https://bookdown.enpchina.eu/HistText_Book/named-entity-recognition-ner.html#ner-on-external-documents) function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545c55d",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(histtext)\n",
    "ner_eng <- ner_on_df(p1, \"Text\", id_column=\"DocId\", model = \"spacy:en:ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a47eeb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Select entities of interest (persons, organizations, locations): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5403170",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pers_eng <- ner_eng %>% filter(Type == \"PERSON\")\n",
    "org_eng <- ner_eng %>% filter(Type == \"ORG\")\n",
    "loc_eng <- ner_eng %>% filter(Type %in% c(\"LOC\", \"GPE\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89ef2e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Persons \n",
    "\n",
    "Data cleaning on persons names. \n",
    "\n",
    "First, remove special characters and digits and convert names to title case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923f367",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_pers_eng <- p1_pers_eng %>% mutate(Text_clean = str_remove_all(Text, \"[:digit:]\")) %>%\n",
    "  mutate(Text_clean = str_replace(Text_clean,\"- \",\"\")) %>%\n",
    "  mutate(Text_clean = str_replace(Text_clean,\" -\",\"\")) %>%\n",
    "  mutate(Text_clean = str_squish(Text_clean))  %>%\n",
    "  mutate(Text_clean = str_remove_all(Text_clean, \"'s\"))%>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean, \"^\\\\.\", \"\")) %>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"^\\\\--\", \"\"))%>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"^\\\\-\", \"\"))%>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"^\\\\'\", \"\"))%>%\n",
    "  mutate(Text_clean2 = str_squish(Text_clean2)) %>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"\\\\''$\", \"\"))%>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"\\\\'$\", \"\"))%>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"\\\\--$\", \"\")) %>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"\\\\-$\", \"\"))%>%\n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \" 'R\", \" R\"))%>% \n",
    "  mutate(Text_clean2 = str_remove_all(Text_clean2, \"''\")) %>% \n",
    "  mutate(Text_clean2 = str_replace_all(Text_clean2, \" '\", \" \")) %>% \n",
    "  mutate(Text_clean2 = str_replace_all(Text_clean2, \"' \", \" \"))%>% \n",
    "  mutate(Text_clean2 = str_replace_all(Text_clean2, \" . \", \" \"))%>%\n",
    "  mutate(Text_clean2 = str_to_title(Text_clean2)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43c94c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Remove titles (Dr, Mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71b612",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_pers_eng <- p1_pers_eng %>% \n",
    "  mutate(Text_clean2 = str_replace(Text_clean2, \"Dr \", \"\"))%>% \n",
    "  filter(!Text_clean2 %in% c(\"Mr\", \"Dr\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb210f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Replace empty strings with NA and remove NA values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2fcd0",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# replace empty strings by NA\n",
    "\n",
    "p1_pers_eng$Text_clean2[p1_pers_eng$Text_clean2==\"\"] <- NA\n",
    "\n",
    "# remove NAs\n",
    "\n",
    "p1_pers_eng <- p1_pers_eng %>% drop_na(Text_clean2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3d445",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Compute basic statistics to assist further data cleaning:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50697751",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# measure text length to identify anomalies and eliminate persons names that contain less than 2 characters\n",
    "\n",
    "p1_pers_eng <- p1_pers_eng %>%  \n",
    "  mutate(length = nchar(Text_clean2)) %>%\n",
    "  filter(length >1)\n",
    "\n",
    "# count names frequency \n",
    "\n",
    "p1_pers_eng <- p1_pers_eng %>%\n",
    "  group_by(Text_clean2) %>% add_tally()\n",
    "\n",
    "# count number of tokens to identify compound names or incomplete names   \n",
    "\n",
    "p1_pers_eng <- p1_pers_eng %>% mutate(token = ntoken(Text_clean2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710af769",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Export dataset to finish manually  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4158d9",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "write.csv(p1_pers_eng, \"p1_pers_eng.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7da9c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Organizations \n",
    "\n",
    "Data cleaning of organizations. \n",
    "\n",
    "First, remove OCR noise and standardize names variants: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc36de",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_org_eng <- p1_org_eng %>% \n",
    "  mutate(Text_clean = str_replace(Text, \"'s$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \" s$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_to_title(Text_clean)) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Mctyeire\", \"McTyeire\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"^The \", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"^He \", \"\"))%>% \n",
    "  mutate(Text_clean = str_squish(Text_clean)) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"^-\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"-$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"^''\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"^'\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"'$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \". ''$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \". '$\", \"\")) %>% \n",
    "  mutate(Text_clean = str_squish(Text_clean)) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"1 Curtis\", \"Curtis\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Co$\", \"Company\"))  %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Co Ltd$\", \"Company\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Co Inc$\", \"Company\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Com Pany\", \"Company\"))  %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Corpora$\", \"Corporation\"))%>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Corp$\", \"Corporation\"))%>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Mfg 5 Mrg\", \"Manufacturing\")) %>% \n",
    "  mutate(Text_clean = str_replace(Text_clean, \"Mfg\", \"Manufacturing\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141b60e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Count length and export for manual refinement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0e6b9",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "p1_org_eng <- p1_org_eng %>% mutate(length = nchar(Text_clean)) \n",
    "write.csv(p1_org_eng, \"p1_org_eng.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49373ba7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Reload cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e00af",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(readr)\n",
    "p1_org_eng_clean <- read_delim(\"p1_org_eng_clean.csv\", \n",
    "                               delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "write.csv(p1_org_eng_clean, \"p1_org_eng_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58d642",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadaa4a7",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "p1_org_eng_clean_count <- p1_org_eng_clean %>% distinct(DocId, Text_clean) %>% group_by(Text_clean) %>% count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7061a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Extract the class of organizations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded1c59",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create vectors of classifiers\n",
    "\n",
    "class <- as.vector(c(\"Association\", \"Club\", \"College\", \"School\", \"University\", \n",
    "                     \"Company\", \"Corporation\", \n",
    "                     \"Army\", \"Corps\", \"Navy\", \n",
    "                     \"Hospital\", \"Clinic\", \n",
    "                     \"Government\", \"Council\", \"Commission\", \"Administration\",\n",
    "                     \"Congress\", \"Parliament\", \"House\",\n",
    "                     \"Press\", \"News\", \"Times\", \"Tribune\",\"Journal\", \"Magazine\", \"Pao\", \"Herald\"))\n",
    "classvec<-unlist(class)\n",
    "classvec2 <- paste(classvec, sep = \"\", collapse = \"|\")\n",
    "\n",
    "# extract the terms listed \n",
    "\n",
    "p1_org_eng_clean <- p1_org_eng_clean %>% mutate(class2 = str_extract(Text_clean, classvec2)) \n",
    "\n",
    "# count number of organizations in each category\n",
    "\n",
    "class_count <- p1_org_eng_clean %>% distinct(Text_clean, class2) %>% group_by(class2) %>% count(sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cd4e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Mapping the Public Sphere (Part 3)\n",
    "\n",
    "## Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40318e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load locations data \n",
    "\n",
    "loc_map <- read_delim(\"maps/loc_map.csv\", \n",
    "                      delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# select countries \n",
    "\n",
    "loc_map_country  <-  loc_map %>% mutate(Country = Place) %>% mutate(Prov_Py = Province) %>% \n",
    "  filter(Type == \"Country\") %>% mutate(Prov_Py = Province) %>% \n",
    "  select(Language, Period, Type, Country, Count)\n",
    "\n",
    "# load packages\n",
    "\n",
    "install.packages(\"tmap\", repos = c(\"https://r-tmap.r-universe.dev\",\n",
    "                                   \"https://cloud.r-project.org\"))\n",
    "library(tmap)    # for static and interactive maps\n",
    "library(leaflet) # for interactive maps\n",
    "library(sf)\n",
    "library(maps)\n",
    "\n",
    "# load country data\n",
    "\n",
    "world_name <- world %>% as.data.frame()\n",
    "world_name <- world_name %>% rename(Country = name_long)\n",
    "\n",
    "# join with my list of countries\n",
    "\n",
    "loc_map_country <- inner_join(loc_map_country, world_name)\n",
    "loc_map_country_eng <- loc_map_country %>% filter(Language == \"English\")\n",
    "\n",
    "# compute mean frequencies across period\n",
    "\n",
    "loc_map_country_eng_mean <- loc_map_country_eng %>% \n",
    "  group_by(Country, iso_a2, geom) %>% \n",
    "  summarise(mean = round(mean(Count), 0)) %>% ungroup()\n",
    "\n",
    "# convert dataframe into shapefile \n",
    "\n",
    "loc_map_country_eng_sf <- st_as_sf(loc_map_country_eng_mean, sf_column_name = \"geom\", crs = \"WGS84\")\n",
    "\n",
    "# Create a color palette for the map:\n",
    "\n",
    "mypalette_eng <- colorBin( palette=\"Blues\", domain=loc_map_country_eng_mean$mean, na.color=\"transparent\", bins = 5)\n",
    "\n",
    "# create choropleth map\n",
    "\n",
    "leaflet(loc_map_country_eng_sf) %>% \n",
    "  addTiles()  %>% \n",
    "  setView( lat=10, lng=0 , zoom=2) %>%\n",
    "  addPolygons( stroke = FALSE, fillOpacity = 0.9, smoothFactor = 0.5, color = ~colorBin(\"Blues\", mean)(mean) ) %>%\n",
    "  addLegend( pal=mypalette_eng, values=~mean, opacity=0.9, title = \"Mean frequency\", position = \"bottomleft\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1217346",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9b12f",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# create city data\n",
    "\n",
    "loc_map_zh_to_join <-  loc_map %>% mutate(Name = Place) %>% mutate(Prov_Py = Province) %>% filter(Country == \"China\")\n",
    "\n",
    "# Chinese cities/provinces\n",
    "\n",
    "MCGD_Data2023.06.21 <- read.csv(\"~/publicsphere/maps/MCGD_Data2023-06-21.csv\")\n",
    "mcgd <- MCGD_Data2023.06.21\n",
    "\n",
    "# join with Chinese coordinates \n",
    "\n",
    "zh_city <- inner_join(loc_map_zh_to_join, mcgd)\n",
    "\n",
    "missing <- setdiff(loc_map_zh_to_join$Name, zh_city$Name)\n",
    "missing <- as.data.frame(missing)\n",
    "\n",
    "write.csv(zh_city, \"zh_city.csv\")\n",
    "\n",
    "# reload clean data\n",
    "\n",
    "zh_city2 <- read_delim(\"maps/zh_city2.csv\", \n",
    "                       delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "# filter non Chinese cities\n",
    "\n",
    "loc_map_cities <-  loc_map %>% mutate(Name = Place) %>% filter(Type == \"City\") %>% filter(!Country == \"China\")\n",
    "\n",
    "# join with geocoordinates\n",
    "\n",
    "library(maps)\n",
    "data(world.cities)\n",
    "\n",
    "world_cities <- world.cities %>% as.data.frame()\n",
    "world_cities2 <- world_cities %>% mutate(name = str_remove_all(name, \"'\"))\n",
    "world_cities2 <- world_cities2 %>% select(Name, country.etc, lat, long)\n",
    "\n",
    "loc_map_cities_latlong <- inner_join(loc_map_cities, world_cities2)\n",
    "loc_map_cities_latlong <- left_join(loc_map_cities, world_cities2)\n",
    "\n",
    "# find duplicates \n",
    "\n",
    "pb <- loc_map_cities_latlong %>% group_by(Name, country.etc) %>% count() %>% filter(n>1)\n",
    "\n",
    "# reload cleaned cities \n",
    "\n",
    "cities_clean <- read_delim(\"cities_clean.csv\", \n",
    "                           delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n",
    "\n",
    "cities_to_join <- cities_clean %>% select(Language, Period, Country, Name, Count, lat, long)\n",
    "\n",
    "zh_city2_to_join <- zh_city2 %>% select(Language, Period, Country, Name, Count, LAT, LONG) %>% rename(lat = LAT, long = LONG)  \n",
    "\n",
    "all_cities <- bind_rows(cities_to_join, zh_city2_to_join)\n",
    "\n",
    "all_cities <- all_cities %>% mutate(City = str_replace(Name, \"Jiangsu\", \"Shanghai\"))\n",
    "\n",
    "# filter by language \n",
    "\n",
    "all_cities_eng <- all_cities %>% filter(Language == \"English\")\n",
    "\n",
    "# compute mean \n",
    "\n",
    "loc_map_city_eng_mean <- all_cities_eng %>% \n",
    "  group_by(Name, lat, long) %>% \n",
    "  summarise(mean = round(mean(Count), 0)) %>% ungroup()\n",
    "\n",
    "# map with leaflet\n",
    "\n",
    "loc_map_city_eng_mean %>%\n",
    "  leaflet() %>%\n",
    "  addTiles() %>%\n",
    "  addCircleMarkers( radius = ~log(mean)*2.5,\n",
    "                    label = ~Name,\n",
    "                    color = \"white\",\n",
    "                    weight = 2,\n",
    "                    opacity = 0.6,\n",
    "                    fill = TRUE,\n",
    "                    fillColor = \"blue\",\n",
    "                    fillOpacity = 0.9,\n",
    "                    stroke = TRUE,\n",
    "                    popup = ~paste( \"City:\", Name ,\n",
    "                                    \"\",\n",
    "                                    \"Mean frequency:\", mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a71a8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Articulating the Public Sphere (Part 4)\n",
    "\n",
    "## Bigrams\n",
    "\n",
    "Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e17a91",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load packages\n",
    "\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "\n",
    "eng_bigram <- doc3_ft %>% \n",
    "  unnest_tokens(output = bigram, \n",
    "                input = Text, \n",
    "                token = \"ngrams\", \n",
    "                n = 2)\n",
    "\n",
    "# preliminary cleaning \n",
    "\n",
    "eng_bigram <- eng_bigram %>% mutate(bigram = str_remove_all(bigram, \"[:digit:]\"))  %>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"[[:punct:]]\", \"\"))%>% \n",
    "  filter(!str_detect(bigram, '[0-9]{1,}')) %>% \n",
    "  mutate(bigram = str_remove_all(bigram, \"'s\")) %>%\n",
    "  mutate(bigram = str_remove_all(bigram, \"'\")) %>%\n",
    "  mutate(bigram = str_remove_all(bigram, \"'\")) %>% ) \n",
    "  mutate(bigram = str_squish(bigram)) %>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"na tions\", \"nations\"))%>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"na tion\", \"nation\")) %>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"na tional\", \"national\"))%>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"inter national\", \"international\"))%>% \n",
    "  mutate(bigram = str_replace_all(bigram, \"inter nationally\", \"internationally\"))%>% \n",
    "  mutate(bigram = str_replace(bigram, \"^ternational\", \"international\"))\n",
    "\n",
    "# separate bigrams into words for further cleaning\n",
    "  \n",
    "eng_bigram_separated <- eng_bigram %>%\n",
    "  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n",
    "\n",
    "# remove stop words and other non words\n",
    "\n",
    "eng_bigram_filtered <- eng_bigram_separated %>% \n",
    "  rename(word = word1) %>%\n",
    "  anti_join(stop_words) %>% \n",
    "  filter(!str_detect(word, '[0-9]{1,}')) %>% \n",
    "  filter(nchar(word) > 3) %>% \n",
    "  filter(!word %in% c(\"tion\", \"tions\", \"chin\", \"ment\")) %>%\n",
    "  mutate(word = str_remove_all(word, \"'s\")) %>%\n",
    "  mutate(word = str_remove_all(word, \"'\")) %>%\n",
    "  rename(word1 = word) %>% \n",
    "  rename(word = word2) %>%\n",
    "  anti_join(stop_words) %>% \n",
    "  filter(!str_detect(word, '[0-9]{1,}')) %>% \n",
    "  filter(nchar(word) > 3) %>% \n",
    "  filter(!word %in% c(\"tion\", \"tions\", \"chin\", \"ment\")) %>%\n",
    "  rename(word2 = word)   \n",
    "\n",
    "# reunite bigrams\n",
    "\n",
    "eng_bigram_united <- eng_bigram_filtered %>%\n",
    "  unite(bigram, word1, word2, sep = \" \")\n",
    "\n",
    "# count bigrams \n",
    "\n",
    "eng_bigram_count <- eng_bigram_united %>%\n",
    "  count(bigram, sort = TRUE)\n",
    "\n",
    "# Join with metadata to identify the most frequent bigrams by period\n",
    "\n",
    "eng_bigram_united_meta <- left_join(eng_bigram_united, meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a6268",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Transnational\n",
    "\n",
    "Focus on bigrams centered on \"international\" and filter irrelevant terms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1495587",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "internat_bigram <- eng_bigram_filtered %>% filter(word1 == \"international\") %>% \n",
    "  filter(!word2 == \"international\") %>% \n",
    "  filter(!word2 %in% c(\"addressed\", \"arrived\", \"approximately\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1b449",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Count frequencies and tf-idf by period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92d39e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "# count bigrams \n",
    "\n",
    "internat_bigram_count <- internat_bigram %>%\n",
    "  count(bigram, sort = TRUE)\n",
    "\n",
    "# tf idf by period\n",
    "internat_bigram_tf_idf_period <- internat_bigram_united_clean %>%\n",
    "  count(period, bigram_clean) %>%\n",
    "  bind_tf_idf(bigram_clean, period, n) %>%\n",
    "  arrange(desc(tf_idf))\n",
    "\n",
    "# visualize \n",
    "\n",
    "internat_bigram_tf_idf_period %>%\n",
    "  group_by(period) %>%\n",
    "  top_n(10, tf_idf) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(bigram_clean = reorder(bigram_clean, tf_idf)) %>%\n",
    "  ggplot(aes(tf_idf, bigram_clean, fill = period)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ period, scales = \"free\", ncol=3) +\n",
    "  labs(x = \"tf-idf\", y = \"bigram\", \n",
    "       title = \"Highest tf-idf bigrams associated with \\\"international\\\" in the Rotary Club corpus\", \n",
    "       subtitle = \"tf-idf by period\", \n",
    "       caption = \"Based on ProQuest \\\"Chinese Newspapers Collection (CNC)\\\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad669d7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Public \n",
    "\n",
    "Focus on bigrams centered on \"public\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20363fa9",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "public_bigram <- eng_bigram_filtered %>% filter(word1 == \"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246cfe5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Count frequencies and tf-idf by period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a836b",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "# count bigrams \n",
    "\n",
    "public_bigram_count <- public_bigram %>%\n",
    "  count(bigram, sort = TRUE)\n",
    "\n",
    "# tf idf by period\n",
    "public_bigram_tf_idf_period <- public_bigram_united_clean %>%\n",
    "  count(period, bigram_clean) %>%\n",
    "  bind_tf_idf(bigram_clean, period, n) %>%\n",
    "  arrange(desc(tf_idf))\n",
    "\n",
    "# visualize \n",
    "\n",
    "public_bigram_tf_idf_period %>%\n",
    "  group_by(period) %>%\n",
    "  top_n(10, tf_idf) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(bigram_clean = reorder(bigram_clean, tf_idf)) %>%\n",
    "  ggplot(aes(tf_idf, bigram_clean, fill = period)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ period, scales = \"free\", ncol=3) +\n",
    "  labs(x = \"tf-idf\", y = \"bigram\", \n",
    "       title = \"Highest tf-idf bigrams associated with \\\"public\\\" in the Rotary Club corpus\", \n",
    "       subtitle = \"tf-idf by period\", \n",
    "       caption = \"Based on ProQuest \\\"Chinese Newspapers Collection (CNC)\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbed04",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Collocates \n",
    "\n",
    "Create collocates (including bigrams) using [quanteda](https://quanteda.io/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971efee6",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "\n",
    "eng_tokens <- tokens(docs3_ft$Text) %>%\n",
    "  tokens_remove(stopwords(\"english\")) \n",
    "eng_colloc <- textstat_collocations(eng_tokens, size = 2, min_count = 2) \n",
    "\n",
    "\n",
    "eng_colloc_clean <- eng_colloc %>% \n",
    "  filter(!str_detect(collocation, '[0-9]{1,}')) %>% \n",
    "  mutate(collocation = str_remove_all(collocation, \"'s\")) %>%\n",
    "  mutate(collocation = str_remove_all(collocation, \"'\")) %>%\n",
    "  mutate(collocation = str_remove_all(collocation, \"'\")) \n",
    "\n",
    "eng_colloc_clean <- eng_colloc %>% mutate(collocation = str_remove_all(collocation, \"[:digit:]\"))  %>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"[[:punct:]]\", \"\")) %>% \n",
    "  mutate(collocation = str_squish(collocation)) %>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"na tions\", \"nations\"))%>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"na tion\", \"nation\")) %>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"na tional\", \"national\"))%>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"inter national\", \"international\"))%>% \n",
    "  mutate(collocation = str_replace_all(collocation, \"inter nationally\", \"internationally\"))%>% \n",
    "  mutate(collocation = str_replace(collocation, \"^ternational\", \"international\"))\n",
    "\n",
    "# replace empty strings by NA\n",
    "\n",
    "eng_colloc_clean$collocation[eng_colloc_clean$collocation==\"\"] <- NA\n",
    "\n",
    "# remove NA \n",
    "eng_colloc_clean <- eng_colloc_clean %>% drop_na(collocation)\n",
    "\n",
    "eng_colloc_clean <- eng_colloc_clean %>% mutate(length = nchar(collocation)) %>% mutate(ntoken = ntoken(collocation))\n",
    "\n",
    "# remove tokens with less than 4 characters\n",
    "\n",
    "eng_colloc_clean <- eng_colloc_clean %>% filter(length > 3) \n",
    "\n",
    "# merge tokens and collocations\n",
    "\n",
    "corpus_tokens <- tokens_compound(eng_tokens, eng_colloc_clean)\n",
    "\n",
    "minimumFrequency <- 2\n",
    "\n",
    "# Create DTM, prune vocabulary and set binary values for presence/absence of types\n",
    "binDTM <- corpus_tokens %>% \n",
    "  tokens_remove(\"\") %>%\n",
    "  dfm() %>% \n",
    "  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = 1000000) %>% \n",
    "  dfm_weight(\"boolean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c0bc2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Convert document-term matrix into tidydaframe to compute pairwise count, clean the data and remove stopwords: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ebe65",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert DTM into tidy dataframe \n",
    "\n",
    "library(dplyr)\n",
    "library(tidytext)\n",
    "\n",
    "eng_td <- tidy(binDTM)\n",
    "\n",
    "# count pairwise \n",
    "\n",
    "library(widyr)\n",
    "\n",
    "word_pairs <- eng_td %>%\n",
    "  pairwise_count(term, document, sort = TRUE)\n",
    "\n",
    "# remove non words based on their length\n",
    "\n",
    "word_pairs <- word_pairs %>% mutate(l1 = nchar(item1)) %>% mutate(l2 = nchar(item2))\n",
    "word_pairs_filtered <- word_pairs %>% filter(l1>3) %>% filter(l2>3)\n",
    "\n",
    "# create list of stopwords to remove \n",
    "\n",
    "stop_words <- c(\"one\", \"even\", \"now\", \"much\", \"can\", \"made\", \"still\", \"may\", \"well\", \"yet\", \"make\", \"first\", \"since\", \n",
    "                \"re\", \"also\", \"said\",\"ing\", \"take\", \"two\", \"every\", \"whole\", \"among\",\"tion\", \"just\", \"however\", \"ed\", \n",
    "                \"last\",\"taken\",\"lo\", \"enough\", \"come\", \"upon\", \"another\", \"without\", \"found\", \"s\", \"done\", \"put\", \"told\", \n",
    "                \"given\",\"might\",\"met\", \"brought\", \"although\",\"gave\", \"less\", \"perhaps\", \"th\",\"ol\",\"pro\", \"led\", \"find\", \"de\", \n",
    "                \"us\", \"ex\", \"ion\", \"tho\", \"nil\", \"certainly\", \"held\", \"arc\", \"toward\", \"rather\", \"bring\", \"china's\", \"con\", \"seen\", \n",
    "                \"see\", \"indeed\", \"get\", \"shown\", \"go\", \"later\", \"stated\", \"really\", \"often\", \"oi\", \"ihe\",\"taking\", \"felt\", \"hi\", \"ho\", \n",
    "                \"strongly\", \"always\", \"used\", \"per\", \"im\", \"lias\", \"showed\", \"look\", \"spoke\", \"says\", \"three\", \"already\", \"able\", \"therefore\", \n",
    "                \"etc\", \"comes\", \"though\", \"especially\", \"ii\", \"due\", \"en\", \"al\", \"cf\", \"pointed\",\"lor\", \"extremely\", \"left\", \"says\", \"tlie\", \"let\", \n",
    "                \"shall\", \"thus\", \"ly\", \"whose\", \"ot\", \"ha\", \"se\", \"dis\", \"ten\", \"latter\", \"third\", \"hold\", \"con_ditions\", \"spite\", \"govern_ment\", \n",
    "                \"owing\", \"hie\", \"ad\", \"un\", \"must\", \"within\",\"went\", \"many\", \"ment\", \"give\")\n",
    "\n",
    "stop_words <- as.data.frame(stop_words)\n",
    "\n",
    "# remove stop words \n",
    "\n",
    "word_pairs_filtered <- word_pairs_filtered %>% filter(!item1 %in% stop_words$stop_words) %>% \n",
    "  filter(!item2 %in% stop_words$stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb8c41",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Transnational\n",
    "\n",
    "Select collocates of \"international\" and \"national\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3ef6b",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "nation_inter <- word_pairs_filtered %>% filter(item1 %in% c(\"national\", \"international\"))\n",
    "nation_inter <- nation_inter %>% select(item1, item2, n) %>% rename(cooc = n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbae1c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Create a node list of words with their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd5d6f",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "nation_inter_node <- nation_inter %>% select(item2) %>% unique() %>% rename(word = item2)\n",
    "word_count <- eng_tidy_token_filtered %>% group_by(word) %>% add_tally() %>% select(word, n) %>% unique()\n",
    "word_count <- word_count %>% rename(count = n)\n",
    "nation_inter_node <- inner_join(nation_inter_node, word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deaba70",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br> \n",
    "Compute word frequencies and join frequencies with the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c640e",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "eng_tidy_token_filtered %>% group_by(word) %>% add_tally() \n",
    "word_count <- eng_tidy_token_filtered %>% select(word, n) %>% unique()\n",
    "word_count <- word_count %>% rename(count = n)\n",
    "nation_inter_node <- inner_join(nation_inter_node, word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5731eb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Filter pairs with weight > 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3405060",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "nation_inter_filtered <- nation_inter %>% filter(cooc > 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b5591",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Build a co-ocurrence network with [igraph](https://r.igraph.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9603f29",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "library(igraph)\n",
    "\n",
    "e.list <- nation_inter_filtered\n",
    "v.attr <- nation_inter_node_filtered\n",
    "\n",
    "G <- graph.data.frame(e.list, vertices=v.attr, directed=FALSE)\n",
    "\n",
    "v.size <- V(G)$count # index node size to words frequencies\n",
    "v.label <- V(G)$name\n",
    "E(G)$weight <- E(G)$cooc # index edge weight to pairs frequencies\n",
    "eigenCent <- evcent(G)$vector # index node color to their eigenvector centrality \n",
    "\n",
    "head(eigenCent, 20)\n",
    "\n",
    "plot(sort(eigenCent, decreasing=TRUE), type=\"l\")\n",
    "\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "V(G)$color <- colorVals\n",
    "\n",
    "l <- layout.fruchterman.reingold(G)\n",
    "\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     edge.width=E(G)$weight*0.01,\n",
    "     vertex.size=v.size*0.01, \n",
    "     vertex.label=v.label,\n",
    "     vertex.label.cex=v.size*0.001,\n",
    "     vertex.label.color=\"black\", \n",
    "     vertex.label.dist=0.2,\n",
    "     vertex.label.family=\"Arial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ac52a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Refine network visualization with [tidygraph](https://tidygraph.data-imaginist.com/) and [gggraph](https://github.com/thomasp85/ggraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98526f",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "library(tidygraph)\n",
    "library(ggraph)\n",
    "\n",
    "tg <- tidygraph::as_tbl_graph(G) %>% activate(nodes) %>% mutate(label=name)\n",
    "\n",
    "v.size <- V(tg)$count\n",
    "E(tg)$weight <- E(tg)$cooc\n",
    "eigenCent <- evcent(tg)$vector\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "\n",
    "\n",
    "tg %>%\n",
    "  ggraph(layout=\"stress\") +\n",
    "  geom_edge_link(alpha = .25, colour='white', aes(width = weight)) +\n",
    "  geom_node_point(size=log(v.size)*2, color=colorVals) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \"lines\"), size=log(v.size), colour=\"white\") +\n",
    "  theme_graph(background = 'grey20')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087886e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Create biplot of collocates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59171e5f",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# select term of interest (international)\n",
    "\n",
    "international <- \"international\" \n",
    "\n",
    "# calculate co-occurrence statistics using log-likelihood (LOGLIK)\n",
    "\n",
    "source(\"https://tm4ss.github.io/calculateCoocStatistics.R\") # load function for co-occurrence calculation\n",
    "\n",
    "coocs <- calculateCoocStatistics(international, DTM, measure=\"LOGLIK\")\n",
    "\n",
    "# reduce document-term matrix \n",
    "\n",
    "redux_dfm <- dfm_select(DTM, \n",
    "                        pattern = c(names(coocs)[1:100], \"international\"))\n",
    "\n",
    "tag_fcm <- fcm(redux_dfm)\n",
    "\n",
    "coocdf <- coocs %>%\n",
    "  as.data.frame() %>%\n",
    "  dplyr::mutate(CollStrength = coocs,\n",
    "                Term = names(coocs)) \n",
    "\n",
    "coocdf_filtered <- coocdf %>%\n",
    "  dplyr::filter(CollStrength > 13) \n",
    "\n",
    "\n",
    "# create matrix of collocates\n",
    "\n",
    "library(Matrix)\n",
    "\n",
    "collocates <- t(binDTM) %*% binDTM\n",
    "collocates <- as.matrix(collocates)\n",
    "\n",
    "coolocs <- c(coocdf_filtered$Term, \"international\")\n",
    "\n",
    "# remove non-collocating terms\n",
    "collocates_redux <- collocates[rownames(collocates) %in% coolocs, ]\n",
    "collocates_redux <- collocates_redux[, colnames(collocates_redux) %in% coolocs]\n",
    "\n",
    "# create biplot of collocates \n",
    "\n",
    "library(FactoMineR)\n",
    "library(factoextra)\n",
    "\n",
    "res.ca <- CA(collocates_redux, graph = FALSE)\n",
    "\n",
    "fviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\", title = \"Collocates of *international* in the Rotary corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36951ced",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Public \n",
    "\n",
    "Select collocates of \"public\" based bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65800934",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# create list of bigrams \n",
    "\n",
    "list <- c(\"public_opinion\", \"public_spirited\", \"public_spirit\",\n",
    "          \"public_service\", \"public_services\", \"public_affairs\", \n",
    "          \"public_welfare\", \"public_utility\", \"public_utilities\")\n",
    "list <- as.data.frame(list)\n",
    "list <- list %>% rename(item1 = list)\n",
    "\n",
    "# select bigrams in list of collocates \n",
    "\n",
    "public_cooc <- word_pairs_filtered %>% filter(item1 %in% list$item1)\n",
    "\n",
    "# filter pairs with a weight > 5 \n",
    "\n",
    "public_cooc <- public_cooc %>% rename(cooc = n) \n",
    "public_filtered <- public_cooc %>% filter(cooc > 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e988975",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Create node list of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255180b",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "public_node_filtered1 <- public_filtered %>% select(item1) %>% unique() %>% rename(word = item1)\n",
    "public_node_filtered2 <- public_filtered %>% select(item2) %>% unique() %>% rename(word = item2)\n",
    "public_node_filtered <- bind_rows(public_node_filtered1, public_node_filtered2)\n",
    "public_node_filtered <- public_node_filtered %>% unique()\n",
    "public_node_filtered <- inner_join(public_node_filtered, word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1689",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Build a co-ocurrence network with [igraph](https://r.igraph.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ae3bb",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "library(igraph)\n",
    "\n",
    "e.list <- public_filtered\n",
    "v.attr <- public_node_filtered\n",
    "\n",
    "G <- graph.data.frame(e.list, vertices=v.attr, directed=FALSE)\n",
    "\n",
    "v.size <- V(G)$count\n",
    "v.label <- V(G)$name\n",
    "E(G)$weight <- E(G)$cooc\n",
    "eigenCent <- evcent(G)$vector\n",
    "\n",
    "head(eigenCent, 20)\n",
    "\n",
    "plot(sort(eigenCent, decreasing=TRUE), type=\"l\")\n",
    "\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "V(G)$color <- colorVals\n",
    "\n",
    "l <- layout.fruchterman.reingold(G)\n",
    "\n",
    "\n",
    "plot(G, \n",
    "     layout=l,\n",
    "     edge.width=E(G)$weight*0.01,\n",
    "     vertex.size=v.size*0.015, \n",
    "     vertex.label=v.label,\n",
    "     vertex.label.cex=v.size*0.002,\n",
    "     vertex.label.color=\"black\", \n",
    "     vertex.label.dist=0.2,\n",
    "     vertex.label.family=\"Arial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8908fa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<br>\n",
    "Refine network visualization with [tidygraph](https://tidygraph.data-imaginist.com/) and [gggraph](https://github.com/thomasp85/ggraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587b998",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "tg <- tidygraph::as_tbl_graph(G) %>% activate(nodes) %>% mutate(label=name)\n",
    "\n",
    "v.size <- V(tg)$count\n",
    "E(tg)$weight <- E(tg)$cooc\n",
    "eigenCent <- evcent(tg)$vector\n",
    "bins <- unique(quantile(eigenCent, seq(0,1,length.out=30)))\n",
    "vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)\n",
    "colorVals <- rev(heat.colors(length(bins)))[vals]\n",
    "\n",
    "\n",
    "tg %>%\n",
    "  ggraph(layout=\"stress\") +\n",
    "  geom_edge_link(alpha = .25, colour='white', aes(width = weight)) +\n",
    "  geom_node_point(size=log(v.size)*1.5, color=colorVals) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \"lines\"), size=log(v.size), colour=\"white\") +\n",
    "  theme_graph(background = 'grey20')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb150b16",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "eval,tags,name,-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
