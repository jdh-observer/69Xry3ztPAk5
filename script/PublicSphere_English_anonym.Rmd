---
title: "Mapping the Transnational Public Sphere in modern China (2)"
subtitle: "Structural Topic Modeling of the English-language press (ProQuest)" 
date: "`r lubridate::today()`"
tags: [bilingual, press, structural topic modeling, transnational]  
abstract: |
  This document is a companion documentation to the paper titled "Mapping the Transnational Public Sphere in Modern China: A Bilingual Topic Modeling of the Republican Press (1919-1949)" to be published in the *Journal of Digital History*. This document focuses on the English-language corpus built from the ProQuest Collection of Chinese Newspapers.    
  
   <style>
    body {
    text-align: justify}
  </style>
    
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
    number_sections: false
    code_folding: show # hide
    fig_caption: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(histtext) 
library(tidyverse)
library(tidytext)
library(tidygraph)
library(ggraph)
library(data.table)
library(quanteda)
library(stm)
library(stminsights)
library(pals)
library(reshape)
library(knitr)
library(kableExtra)
```

# Research context 

This paper seeks to investigate the formation of a transnational public sphere in republican China, through the joint empirical study of two key institutions – a non-state transnational organization, the Rotary Club, and its representations in the Shanghai press, which has long been considered as a key medium for shaping and disseminating information in modern China (Rankin, 1990; Huang, 1993; Wakeman, 1993; Wagner, 2007). Previous research on the Chinese public sphere presents to main limitations. One the one hand, scholars have focused on theoretical discussions regarding the transferability of Western concepts in China, instead of examining its concrete manifestations in the press and how it was put in practice by social actors. On the other hand, scholars who have used the press as a source have essentially relied on the close reading of subjectively selected articles, without providing the possibility to contextualize their findings and to assess whether/to what extent the selected texts or passages were representative of larger trends. 

Taking advantage of the massive, multilingual corpora recently made available in full text by the [ENP-China (Elites, Networks and Power in modern China) project](https://www.enpchina.eu/), this paper introduces a mixed-method approach based on topic modeling to enable a change of scale in the analysis of the historical press and to overcome certain limitations of manual reading.Topic modeling is a computational, statistical method aimed at automatically detecting hidden themes (topics) in large collections of unstructured texts, based on the co-occurrences of words in documents. In this paper, we rely on structural topic modeling (STM). STM is based on Latent Dirichlet Allocation (LDA), a probabilistic model that treats topics as mixtures of words and documents as mixtures of topics. This implies that words can belong to different topics, while topics can be represented in multiple documents with varying proportions. In addition, STM is able to incorporate document metadata such as the date of publication, which enables to analyze topical changes over time. More specifically, we will use the [stm R package](https://warin.ca/shiny/stm/) which includes several built-in functions designed to facilitate the exploration of topics, including various visualizations and statistical outputs. 

The purpose of this research is twofold. Substantively, our key questions include: How did the Shanghai press reported on the Rotary Club? How did the organization mediate between elite/society, business/politics, localism/internationalism? What does this reveal about how the press itself functioned as a public sphere? How did this emerging public sphere change over time and vary across languages? Methodologically, we aim to design a reliable method for conducting a bilingual, dynamic topic modeling approach of the historical press. More specifically, we address three major challenges: (1) to identify topics across multiple languages (in this paper, English and Chinese), (2) to trace topical changes over time, and (3) to adapt topic modeling to the heterogeneity of newspaper content, particularly to brevity-style articles made up of short pieces of unrelated news. 

In this document, we focus on the English-language press, based on the [ProQuest Chinese Newspapers Collection (CNC)](https://about.proquest.com/en/products-services/hnp_cnc). For the Chinese-language press (Shenbao), see the counterpart document. Our workflow follows four main steps. First, we build the corpus from the ENP-China textbase using the [HistText package](https://bookdown.enpchina.eu/rpackage/HistTextRManual.html). Second, we prepare the text data and build the topic models using the [stm package](https://www.structuraltopicmodel.com/). Next, we explore and label the topics using various visualizations and statistical measures. Finally, we analyze the effect of time on topic prevalence based on the date of publication. 

Note: The purpose of this document is to describe our workflow and to make our methodological choices more explicit, testable  and replicable. Historical questions and interpretations are kept to the minimum. For a comprehensive literature review and detailed interpretation of the findings embedded in the final narrative, see to the companion research paper to be published in the [*Journal of Digital History* (JDH)](https://journalofdigitalhistory.org/en). 

# Corpus building

Load packages
```{r}
library(histtext)
library(tidyverse)
```

<br>
We first search the "Rotary Club" in the ProQuest collection. Since we are investigating a very specific organization with few possible homonyms and low degree of ambiguity, we can rely on simple keywords. We only restrict the query to the period posterior to 1919, when the first Rotary Club in China was established in Shanghai: 
```{r}

rotary_eng_docs <- search_documents_ex('"rotary club"', corpus= "proquest", 
                      filter_query = c("publisher", "category"),
                      dates="[1919 TO 1949]", 
                      verbose = FALSE)

head(rotary_eng_docs)

```
<br>
When we retrieved the full text of the documents (not done here), we realized that the results contained many articles in which the Rotary Club was just mentioned in passing, amidst unrelated pieces of news. Using the entire document as a text unit would only reflect the messy structure of these texts. In order to alleviate the issue, we propose to apply topic modeling on finer segments of text instead of the entire document. 

Example of problematic documents: 

```{r}
view_document(1319907286, "proquest", query = '"rotary club"')
```
<br>
In the above example, the length of the targeted segment is 38 words, whereas the total length of the “article” is 2091 words.  

## Retrieve concordance

Instead of retrieving entire documents, therefore, we will retrieve finer strings of characters using the "concordance" function included in the histtext package. This function returns the queried terms in their context. The main challenge at this stage is to define the right context size. After a careful examination of a sample of articles, we decided to set the threshold at 400 characters to minimize the risk of overlap in cases when articles contain several occurrences of the queried terms: 

```{r}

rotary_eng_conc400 <- histtext::search_concordance_ex('"rotary club"', corpus = "proquest", 
                      filter_query = c("publisher", "category"),
                      dates="[1919 TO 1949]", 
                      context_size = 400, verbose = FALSE) 

head(rotary_eng_conc400)

```

<br>
The concordance table contains seven columns, including the unique identifier of the document (DocId), the date of publication, the title of the article (Title), the name of the periodical (Source), the queried terms (Matched), and the terms preceding (Before) and following (After) the key words. 

## Filter collections

In the next step, we remove the Hong Kong-based periodical *South China Morning Post* to focus on mainland periodicals: 
```{r}

rotary_eng_conc400_filtered <- rotary_eng_conc400 %>% 
  filter(!Source %in% c("South China Morning Post Publishers Limited", "South China Morning Post Ltd.")) %>% 
  mutate(Year = stringr::str_sub(Date,0,4)) %>% 
  mutate(Year = as.numeric(Year)) %>% 
  filter(Year > 1918) 

head(rotary_eng_conc400_filtered)

```
<br>
The filtered corpus contains 3,620 instances and 2,468 documents. 

Next, we retrieve the genre of article as metadata, which will be used later to filter out irrelevant contents. Since the category of article can not be directly retrieved with the "search_concordance_ex" function in the current version of histtext, we need to use the function "get_search_field_content" instead. 
```{r}

rotary_eng_doc <- histtext::get_search_fields_content(rotary_eng_conc400_filtered, "proquest",
                                                     search_fields = c("title", "date", "publisher", "category", "fulltext"),
                                                     verbose = FALSE)

head(rotary_eng_doc)

```
<br>
In the next steps, we retain only unique documents, we discard the variables that are redundant with the concordance table (except for "DocId" which will be used for joining the two tables) and we clean the content of the "category" field: 
```{r}

rotary_eng_doc <- rotary_eng_doc %>% 
  unique() %>%
  select(DocId, category) %>% 
  mutate(category = str_remove_all(category,"\\]")) %>% 
  mutate(category = str_remove_all(category,"\\[")) %>% 
  mutate(category = str_remove_all(category,"\\'"))

head(rotary_eng_doc)

```
<br>
Let's inspect the various categories and their relative importance in our corpus: 
```{r}

rotary_eng_doc %>% select(DocId, category) %>% 
  group_by(category) %>% 
  count(sort = TRUE)

```
<br>
On this basis, we decide to filter out the following non-significant categories: 
```{r}
rotary_eng_doc_filtered <- rotary_eng_doc %>% filter(!category %in% c("Advertisement", 
                                                                    "Classified Advertisement, Advertisement", 
                                                                    "General Information", 
                                                                    "Table of Contents, Front Matter"))
```
<br>
The resulting corpus contains 2387 documents. We can then join the list of document with the concordance table: 
```{r}
rotary_join <- inner_join(rotary_eng_doc_filtered, rotary_eng_conc400_filtered)

head(rotary_join)
```
<br> 
The concordance table now includes the category of article among the metadata. 

## Create text units

In the next steps, we will transform the text data so that it can be processed by the topic model. 

First, we create a new variable "Text" for the merged text: 
```{r}

# We first merge "Before" and "Matched" into a new "Text" variable
rotary_merged <- rotary_join %>% mutate(Text = paste(Before, Matched, sep = " ")) 
head(rotary_merged)

# Second, we merge "After" with the text resulting from previous operation (Text): 
rotary_merged <- rotary_merged %>% mutate(Text = paste(Text, After, sep = " ")) 
head(rotary_merged)
```
<br>
Finally, we re-unite each document to include all the occurrences it may contain, and we retain only one instance for each document:
```{r}

library(data.table)

rotary_united <- rotary_merged %>%
  group_by(DocId, Date, Year, Source, category, Title, grp = rleid(DocId)) %>% 
  summarise(Text = str_c(Text, collapse=' '), .groups = 'drop') %>%
  ungroup %>%
  select(-grp)

head(rotary_united)

```
<br>
Our final corpus contains 2,387 reshaped documents spanning from 1919 to 1948: 
```{r}
rotary_united %>% 
  group_by(Year) %>% count(Source) %>%
  ggplot(aes(x=Year, y=n, fill=Source)) + 
  geom_col(alpha = 0.8) + 
  labs(title = "The Rotary Club in the English-language press",
       subtitle = "Number of articles mentioning the club",
       x = "Year", 
       y = "Number of articles",
       caption = "Based on ProQuest Chinese Newspaper Collection") 
```
<br>
The peak in the 1930s largely reflects the increase in the volume of periodicals during this period. It also coincides with the growth of Chinese memberships and the most active period in the history of the Rotary Club. It was followed by a dramatic decline during the Sino-Japanese war (1937-1945) during which most foreign periodicals ceased publications and never fully recovered afterwards.  

In addition, we observe significant differences between periodicals. Three major publications, in fact, dominated the corpus: 
```{r}

rotary_united %>% 
  group_by(Source)  %>%  
  summarise(n = n()) %>%
  mutate(ptg = paste0(round(n / sum(n) * 100, 0), "%")) %>%
  arrange(desc(n))

```
<br> 
Before we move to the next step (text pre-processing), we recommend saving the dataset as a csv file: 
```{r eval = FALSE}
write.csv(rotary_united, "rotary_eng_conc400.csv")
```

# Pre-processing 

Next, we need to prepare the text data to make it readable by topic model algorithms.

In the pre-processing phase, we chose not to stem and not lemmatize words because at this stage, we wanted to maintain all possible nuances conveyed in the original texts. We removed words which contained less than four characters and occurred in less than five documents. We also removed a customized list of stop words, especially the terms used to query the corpus, as well as high-frequency terms in this context, such as "China" and "Chinese". Based on these parameters, 14517 out of 16895 terms (22589 of 82569 tokens) were removed due to frequency. The final corpus contains 2,387 documents, 2378 terms and 59980 tokens. 
```{r}

meta <- rotary_united %>% transmute(DocId, Date, Year, Source, category, Title)

corpus <- stm::textProcessor(rotary_united$Text,
                             metadata = meta, 
                             stem = FALSE, 
                             wordLengths = c(4, Inf), 
                             verbose = FALSE, 
                             customstopwords = c("rotary", "club", "china", "chinese", "will", "one", "two"))

stm::plotRemoved(corpus$documents, lower.thresh = c(0,10, by=5))

out <- stm::prepDocuments(corpus$documents, 
                          corpus$vocab, 
                          corpus$meta, 
                          lower.thresh = 5)
```
<br>
Before we move on towards building the models, a sound reflex is to inspect more closely which words were removed:
```{r}
wordsremoved <- as_tibble(out$words.removed) 
wordsremoved
```
<br>
Notice that many of the removed words include errors in optical character recognition (OCR) and other noisy contents resulting from the process of digitization.  

# Model building

Choosing the right number of topics *k* remains a highly debated question. There is no definite solution. Most topic modeling tools generally provide a set of metrics such as held-out likelihood, residual analysis, average exclusivity and semantic coherence, to help the researcher to determine the optimal number of topics for a given corpus. According to the authors of the [manual of the stm package](https://cran.r-project.org/web/packages/stm/stm.pdf), for small corpora ranging from a few hundred to a few thousand documents, the best number of topics should range between 5 and 50 topics ^[Margaret Roberts et al., “Stm: Estimation of the Structural Topic Model,” September 18, 2020, 64–65, https://CRAN.R-project.org/package=stm]. Ultimately, however, only the researcher’s interpretational needs can determine what is the most appropriate number of topics for a given specific research. 

In the stm R package, the *searchK* function provides a wide range of metrics to guide our choice, including held-out likelihood, residual analysis, average exclusivity and semantic coherence. Only the default properties (held-out likelihood, residuals, semantic coherence, lower bound) are displayed below: 
```{r}
set.seed(1111)
K<-seq(5,50, by=10) 
kresult <- searchK(out$documents, out$vocab, K,  data=out$meta, prevalence =~ Year + Source + category, verbose=FALSE)
plot(kresult)
```
<br>
After several experiments, we decided build three models with 5, 10 and 20 topics, which will enable us to navigate different levels of granularity: 
```{r}

# 5-topic model
mod.5 <- stm::stm(out$documents, 
                   out$vocab, K=5, 
                   data=out$meta, 
                   prevalence =~ Year + Source + category, 
                   verbose = FALSE)

# 10-topic model
mod.10 <- stm::stm(out$documents, 
                   out$vocab, K=10, 
                   data=out$meta, 
                   prevalence =~ Year + Source + category, 
                   verbose = FALSE)

# 20-topic model
mod.20 <- stm::stm(out$documents, 
                   out$vocab, K=20, 
                   data=out$meta, 
                   prevalence =~ Year + Source + category,  
                   verbose = FALSE)
```

<br>
Finally, we incorporate the time variables in the models to further analyze topical changes over time: 
```{r}
year5 <- stm::estimateEffect(1:5 ~ Year, mod.5, meta=out$meta)
year10 <- stm::estimateEffect(1:10 ~ Year, mod.10, meta=out$meta)
year20 <- stm::estimateEffect(1:20 ~ Year, mod.20, meta=out$meta)
```
<br>
Additionally, we can incorporate other relevant metadata such as the source (name of periodical) and the genre of article (category), if we want to investigate differences in topical contents between periodicals or between categories of articles.   

Estimate source effect: 
```{r}
source5 <- stm::estimateEffect(1:5 ~ Source, mod.5, meta=out$meta)
source10 <- stm::estimateEffect(1:10 ~ Source, mod.10, meta=out$meta)
source20 <- stm::estimateEffect(1:20 ~ Source, mod.20, meta=out$meta)
```
<br>
Estimate source effect: 
```{r}
category5 <- stm::estimateEffect(1:5 ~ category, mod.5, meta=out$meta)
category10 <- stm::estimateEffect(1:10 ~ category, mod.10, meta=out$meta)
category20 <- stm::estimateEffect(1:20 ~ category, mod.20, meta=out$meta)
```

Finally, we save the models as an ".RData" file to save time and computational power in the future: 
```{r eval = FALSE}
save.image('rotaryeng.RData')
```

# Model evaluation 

To compare the three models, we can plot the semantic coherence of topics against their exclusivity. As the plot suggested, the higher the number of topics, the lower their semantic coherence, and the higher their exclusivity:   
```{r}

mod5df<-as.data.frame(cbind(c(1:5),exclusivity(mod.5), semanticCoherence(model=mod.5, out$documents), "PQ5T"))
mod10df<-as.data.frame(cbind(c(1:10),exclusivity(mod.10), semanticCoherence(model=mod.10, out$documents), "PQ10T"))
mod20df<-as.data.frame(cbind(c(1:20),exclusivity(mod.20), semanticCoherence(model=mod.20, out$documents), "PQ20T"))

models<-rbind(mod5df, mod10df, mod20df)
colnames(models)<-c("Topic","Exclusivity", "SemanticCoherence", "Model")

models$Exclusivity<-as.numeric(as.character(models$Exclusivity))
models$SemanticCoherence<-as.numeric(as.character(models$SemanticCoherence))

options(repr.plot.width=7, repr.plot.height=6, repr.plot.res=100)

plotmodels <-ggplot(models, aes(SemanticCoherence, Exclusivity, color = Model))+
  geom_point(size = 2, alpha = 0.7) + 
  geom_text(aes(label=Topic), nudge_y=.04)+
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence", 
       subtitle = "English-language corpus (ProQuest)")


plotmodels
```

# Model exploration

In the first step, we highly recommend using the package [stminsights](https://cran.r-project.org/web/packages/stminsights/vignettes/intro.html) to explore the models:  
```{r eval = FALSE}
library(stminsights)
run_stminsights()
```
<br>
[Stminsights](https://cran.r-project.org/web/packages/stminsights/vignettes/intro.html) is an R Shiny application which provides a set of visualizations and statistical tools for exploring the topics in one or across multiple models. While building on the stm package itself, it greatly facilitates the preliminary exploration. In the next sections, we shall provide the full code for reproducing and adjusting the outputs produced through the "stm insights" application. 

## Topic proportions θ 

The package "stm" stores the document-topic proportions and the topic-word distributions in two matrices, θ (which is also referred to, somewhat confusingly, as γ) and β. We can then take a closer look at θ, which can be called directly from the model. Alternatively, it is possible and perhaps more convenient to use the built-in function "make.dt()". The latter allows to incorporate the metadata, which in our case is helpful since we aim to examine the influence of the data of publication in topic prevalence. The table below display the proportions of topics for each document, along with their metadata. 

Extract topic proportions for each model: 
```{r}

topicprop5<-make.dt(mod.5, meta)
topicprop10<-make.dt(mod.10, meta)
topicprop20<-make.dt(mod.20, meta)

```
<br>
Consulting the table might be a bit cumbersome unless we want to examine the topic proportions of a specific document. The "plot.STM" function associated with the "hist" argument helps to better visualize the estimates of document-topic proportions:
```{r}
plot.STM(mod.5, "hist")
plot.STM(mod.10, "hist")
plot.STM(mod.20, "hist")
```
<br>
Next, we can examine more closely the words that define the topics in order to better understand what each topic is really about.

## Word per topic *β*

In the stm package, the function "plot.STM" with argument "summary" displays the general distribution of topics (which topics are overall more common in the corpus) along with the most common words for each topic. In the example below, we set the number of desired words to 5:  
```{r warning=FALSE, message=FALSE}
plot.STM(mod.5,"summary", n=5)
plot.STM(mod.10, "summary", n=5)
plot.STM(mod.20, "summary", n=5)
```
<br>
Alternatively, we can plot words proportions per topics as bar plots using a tidy approach. In the example below, we focus on the 10-topic model. The plots display the 10 most frequent words for each topic: 
```{r}
# load packages

library(tidyverse)
library(tidytext)

# extract β proportions for the 10-topic model 

td_beta10_eng <- tidytext::tidy(mod.10) 

# plot the the distribution of words (10 first words) for each topic 

options(repr.plot.width=7, repr.plot.height=8, repr.plot.res=100) 

td_beta10_eng %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics",
       caption = "Based on ProQuest Chinese Newspapers Collection")

```

<br>
Explore the top 10 words for each topic: 
```{r}
topic10eng_top_words <- td_beta10_eng %>%
  group_by(topic) %>%
  top_n(10, beta) 

topic10eng_top_words %>% arrange(topic, desc(beta))

```


## Topic labeling 

The function "labelTopics" (or sageLabels) provides a deeper insight on the popular words in each topic. In addition to words probability, other metrics can be computed, including the FREX words (FREX weights words by frequency and exclusivity to the topic), lift words (frequency divided by frequency in other topics), and score (similar to lift, but with log frequencies). In the example below, we set the number of words to 10: 

```{r}
labelTopics(mod.5, n=10)
labelTopics(mod.10, n=10)
labelTopics(mod.20, n=10)
```
<br>
For example, we can display the 10 FREX words for the three topics that mixed local and international concerns in the 10-topic model: 
```{r}
plot.STM(mod.10, "labels", topics=c(1,5,10), label="frex", n=10, width=50)
```

## Word clouds

Word clouds provide a more intuitive, though less rigorous way of visualizing word prevalence in topics. The example below displays the word clouds of the same three topics as above: 
```{r}
cloud(mod.10, topic = 1, scale = c(4, 0.4))
cloud(mod.10, topic = 5, scale = c(4, 0.4))
cloud(mod.10, topic = 10, scale = c(4, 0.4))
```

## Quotations

Some topics may still remain unclear and require that we have a closer look at a sample of documents in order to better understand what they are about. The "findThoughts" function enables us to have a glimpse at the most representative documents for each topic, and plot them with the function "plotQuote". 

```{r}

T10_thoughts1 <- findThoughts(mod.10,texts=rotary_united$Text, topics=1, n=5)$docs[[1]]
T10_thoughts5 <- findThoughts(mod.10,texts=rotary_united$Text, topics=5, n=5)$docs[[1]]
T10_thoughts10 <- findThoughts(mod.10,texts=rotary_united$Text, topics=10, n=5)$docs[[1]]

par(mfrow=c(1,3), mar=c(1,1,2,2))
plotQuote(T10_thoughts1, width=50, maxwidth=400, text.cex=0.5, main="Topic 1")
plotQuote(T10_thoughts5, width=50, maxwidth=400, text.cex=0.5, main="Topic 5")
plotQuote(T10_thoughts10, width=50, maxwidth=400, text.cex=0.5, main="Topic 10")

```

## Topic correlation 

Topic correlation helps to better structure the exploration of the model. It shows relations between topics based on the proportions of words they have in common. The "stm" package provides two options for estimating topic correlations. The "simple" method simply thresholds the covariances whereas the "huge" method uses the semi-parametric procedure in the package. Let’s compare the two procedures: 
```{r}
corrsimple <- topicCorr(mod.20, method = "simple", verbose = FALSE)
corrhuge <- topicCorr(mod.20, method = "huge", verbose = FALSE)
par(mfrow=c(1,2), mar=c(0,0,2,2))
plot(corrsimple, main = "Simple method")
plot(corrhuge, main = "Huge method")
```
<br>
We can further use the package [gggraph](https://exts.ggplot2.tidyverse.org/ggraph.html) to get visualize with greater precision the topic proportions and the weights of correlation. Let’s start with the simple method:

```{r}
# extract network 
stm_corrs <- get_network(model = mod.20,
                         method = 'simple',
                         labels = paste('Topic', 1:20),
                         cutoff = 0.001,
                         cutiso = TRUE)

# plot network with ggraph 
library(ggraph)

ggraph(stm_corrs, layout = 'fr') +
  geom_edge_link(
    aes(edge_width = weight),
    label_colour = '#fc8d62',
    edge_colour = '#377eb8') +
  geom_node_point(size = 4, colour = 'black')  +
  geom_node_label(
    aes(label = name, size = props),
    colour = 'black',  repel = TRUE, alpha = 0.85) +
  scale_size(range = c(2, 10), labels = scales::percent) +
  labs(size = 'Topic Proportion',  edge_width = 'Topic Correlation', title = "Simple method") + 
  scale_edge_width(range = c(1, 3)) +
  theme_graph()
```

## Interactive visualization

The "stm" package also includes a function "LDAvis" which produces an interactive visualization of an LDA topic model. The main graphical elements include:

  * Default topic circles - K circles, one for each topic, whose areas are set to be proportional to the proportions of the topics across the N total tokens in the corpus.
  * Red bars - represent the estimated number of times a given term was generated by a given topic.
    * Blue bars - represent the overall frequency of each term in the corpus
    * Topic-term circles - K×W circles whose areas are set to be proportional to the frequencies with which a given term is estimated to have been generated by the topics.
    
```{r eval = FALSE}
stm::toLDAvis(mod.5, doc=out$documents)
stm::toLDAvis(mod.10, doc=out$documents)
stm::toLDAvis(mod.20, doc=out$documents)
```
<br>
In the next section, we propose to analyze the effect of time (date of publication) on topic prevalence in the three models. 

# Topics over time

First, select topic proportions  
```{r}
topic5prop <- topicprop5 %>% select(c(2:6))
topic10prop <- topicprop10 %>% select(c(2:11))
topic20prop <- topicprop20 %>% select(c(2:21))
```
<br>
Compute topic proportions per year 
```{r}
topic_proportion_per_year5 <- aggregate(topic5prop, by = list(Year = rotary_united$Year), mean)
topic_proportion_per_year10 <- aggregate(topic10prop, by = list(Year = rotary_united$Year), mean)
topic_proportion_per_year20 <- aggregate(topic20prop, by = list(Year = rotary_united$Year), mean)
```
<br>
Reshape data frame
```{r}
library(reshape)
vizDataFrame5y <- melt(topic_proportion_per_year5, id.vars = "Year")
vizDataFrame10y <- melt(topic_proportion_per_year10, id.vars = "Year")
vizDataFrame20y <- melt(topic_proportion_per_year20, id.vars = "Year")
```
<br>
Plot topic proportions per year as bar plots: 
```{r}
library(pals)

# 5-topic model: 
ggplot(vizDataFrame5y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="The Rotary Club in the English-language press", 
       subtitle = "Topic proportion over time (5-topic model)")

# 10-topic model:
ggplot(vizDataFrame10y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="The Rotary Club in the English-language press", 
       subtitle = "Topic proportion over time (10-topic)")

# 20-topic model:
ggplot(vizDataFrame20y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  labs(title="The Rotary Club in the English-language press", 
       subtitle = "Topic proportion over time (20-topic)")

```

# Concluding remarks 

Methodologically, the contribution of this paper is threefold: 

  1. First, it has offered a simple yet efficient solution, based on concordance, to the problem of article segmentation in digitized newspapers. This preliminary method needs to be refined and adjusted to different types of texts depending on their varying length and relevance to the research question. 
  2. Second, this paper constitutes a rare instance of cross-lingual comparison involving the Chinese language during its transitional stage between classical and modern Chinese. As a low-resource language, pre-modern Chinese presents significant challenges for the application of natural language processing tools and computer-assisted text analysis. As E. Kaske has demonstrated (Kaske, 2007), the Chinese language was highly unstable during the century under study, which was a pivotal phase in the creation of a standard vernacular (baihua) national language (guoyu). In his data-driven study of the *Shenbao*, P. Magistry has shown that the Chinese language actually evolved through six main stages between 1872 and 1949. Further research should pay greater attention to the decisions made during the pre-processing phase and design strict protocols for evaluating the impact of tokenization and language variation on the resulting topics. From a multilingual perspective, future research could also benefit from more sophisticated techniques for the automatic alignment of topics across languages.  It could also investigate the differences between the various English periodicals included in the ProQuest collection, especially between the British North China Herald, the American China Weekly Review, and the Chinese-owned China Press. 
  3. Third, this research has demonstrated the value of combining different models with different k number of topics, instead of focusing on a single, definitive model. This multi-model approach is particularly appropriate when dealing with corpora of different sizes and with different structures. This multi-scalar reading of corpora enables scholars to navigate between different levels of granularity and to select in each model the topics that are the most relevant to the research question. 
  
The results of this topic modeling exercise can be used as a starting point for addressing more specific research questions. The inferred topics point to the existence of two main categories of articles that can be further investigated using adequate methods. On the one hand, topics related to meetings, organization, and philanthropy are generally rich with names of individuals, organizations, and locations. Named entity recognition (NER) and network analysis can then be utilized to automatically extract the names of these actors and further analyze their connections. On the other hand, topics related to lectures and discussions (forums), which are richer in semantic contents, lend themselves to a deeper examination of the discourses articulated by the various actors, using methods such as semantic and sentiment analysis. Finally, while the Rotary Club has served as a test case in this paper, our methodology can be expanded to investigate other public sphere institutions and more abstract concepts related to the public sphere. Furthermore, it can be transposed to similar digitized texts in English, Chinese, and possibly other languages, beyond the specific corpora utilized in this research. 

# References 

Armand, Cécile. “Foreign Clubs with Chinese Flavor: The Rotary Club of Shanghai and the Politics of Language.” In Knowledge, Power, and Networks: Elites in Transition in Modern China, edited by Cécile Armand, Christian Henriot, and Huei-min Sun, 233–59. Leiden: Brill, 2022.
Huang, Philip C.C. “‘Public Sphere “/”Civil Society’ in China? The Third Realm between State and Society.” Modern China Modern China 19, no. 2 (1993): 216–40.
Kaske, Elisabeth. The Politics of Language in Chinese Education, 1895-1919. Vol. 82. Sinica Leidensia. Leiden: Brill, 2007.
Magistry, Pierre. “Languages(s) of the Shun-Pao, a Computational Linguistics Account.” In 10th International Conference of Digital Archives and Digital Humanities. Taipei, Taiwan, 2019.
Rankin, Mary Backus. “The Origins of a Chinese Public Sphere. Local Elites and Community Affairs in the Late Imperial Period.” Études Chinoises 9, no. 2 (1990): 13–60.
Wagner, Rudolf G, ed. Joining the Global Public: Word, Image, and City in Early Chinese Newspapers, 1870-1910. Albany, NY: State University of New York Press, 2007.
Wakeman, Frederic. “The Civil Society and Public Sphere Debate: Western Reflections on Chinese Political Culture.” Modern China 19, no. 2 (1993): 108–38.

# Acknowledgements

This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 788476) and a CCFK grant.