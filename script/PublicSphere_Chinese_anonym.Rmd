---
title: "Mapping the Transnational Public Sphere in modern China (1)"
subtitle: "Structural Topic Modeling of the *Shenbao*" 
date: "`r lubridate::today()`"
tags: [bilingual, press, structural topic modeling, transnational]  
abstract: |
   This document is a companion documentation to the paper titled "Mapping the Transnational Public Sphere in Modern China: A Bilingual Topic Modeling of the Republican Press (1919-1949)" to be published in the *Journal of Digital History*. This document focuses on the Chinese-language corpus built from the newspaper *Shenbao*.   
  
  <style>
    body {
    text-align: justify}
  </style>
    
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
    number_sections: false
    code_folding: show # hide
    fig_caption: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(histtext) 
library(tidyverse)
library(tidytext)
library(tidygraph)
library(ggraph)
library(data.table)
library(jiebaR)
library(jiebaRD)
library(quanteda)
library(stm)
library(stminsights)
library(pals)
library(reshape)
library(knitr)
library(kableExtra)
```

# Research context 

This paper seeks to investigate the formation of a transnational public sphere in republican China, through the joint empirical study of two key institutions – a non-state transnational organization, the Rotary Club, and its representations in the Shanghai press, which has long been considered as a key medium for shaping and disseminating information in modern China (Rankin, 1990; Huang, 1993; Wakeman, 1993; Wagner, 2007). Previous research on the Chinese public sphere presents to main limitations. One the one hand, scholars have focused on theoretical discussions regarding the transferability of Western concepts in China, instead of examining its concrete manifestations in the press and how it was put in practice by social actors. On the other hand, scholars who have used the press as a source have essentially relied on the close reading of subjectively selected articles, without providing the possibility to contextualize their findings and to assess whether/to what extent the selected texts or passages were representative of larger trends. 

Taking advantage of the massive, multilingual corpora recently made available in full text by the [ENP-China (Elites, Networks and Power in modern China) project](https://www.enpchina.eu/), this paper introduces a mixed-method approach based on topic modeling to enable a change of scale in the analysis of the historical press and to overcome certain limitations of manual reading.Topic modeling is a computational, statistical method aimed at automatically detecting hidden themes (topics) in large collections of unstructured texts, based on the co-occurrences of words in documents. In this paper, we rely on structural topic modeling (STM). STM is based on Latent Dirichlet Allocation (LDA), a probabilistic model that treats topics as mixtures of words and documents as mixtures of topics. This implies that words can belong to different topics, while topics can be represented in multiple documents with varying proportions. In addition, STM is able to incorporate document metadata such as the date of publication, which enables to analyze topical changes over time. More specifically, we will use the [stm R package](https://warin.ca/shiny/stm/) which includes several built-in functions designed to facilitate the exploration of topics, including various visualizations and statistical outputs. 

The purpose of this research is twofold. Substantively, our key questions include: How did the Shanghai press reported on the Rotary Club? How did the organization mediate between elite/society, business/politics, localism/internationalism? What does this reveal about how the press itself functioned as a public sphere? How did this emerging public sphere change over time and vary across languages? Methodologically, we aim to design a reliable method for conducting a bilingual, dynamic topic modeling approach of the historical press. More specifically, we address three major challenges: (1) to identify topics across multiple languages (in this paper, English and Chinese), (2) to trace topical changes over time, and (3) to adapt topic modeling to the heterogeneity of newspaper content, particularly to brevity-style articles made up of short pieces of unrelated news. 

In this document, we focus on the Chinese-language newspaper *Shenbao* (Chinese)^[*Shenbao* was a leading newspaper published in Shanghai between 1872 and 1949. Despite low literacy rates among the Chinese population, it reached 150,000 copies in the 1930s, making one of the two most widely circulated newspapers in China. Although it catered primarily to Shanghai intellectual, political, and business elites, its readership widened in the 1930s. Although it was printed in Shanghai, *Shenbao* had a national, even international coverage, being also circulated among overseas Chinese]. For the English-language press (ProQuest Collection of Chinese Newspapers), see the counterpart document. Our workflow follows four main steps. First, we build the corpus from the ENP-China textbase using the [HistText package](https://bookdown.enpchina.eu/rpackage/HistTextRManual.html). Second, we prepare the text data and build the topic models using the [stm package](https://www.structuraltopicmodel.com/). Next, we explore and label the topics using various visualizations and statistical measures. Finally, we analyze the effect of time on topic prevalence based on the date of publication. 

Note: The purpose of this document is to describe our workflow and to make our methodological choices more explicit, testable  and replicable. Historical questions and interpretations are kept to the minimum. For a comprehensive literature review and detailed interpretation of the findings embedded in the final narrative, see to the companion research paper to be published in the [*Journal of Digital History* (JDH)](https://journalofdigitalhistory.org/en). 

# Corpus building

Load packages
```{r}
library(histtext)
library(tidyverse)
```
<br>
We first search "扶輪社" (fulunshe) in the *Shenbao*. Since we are investigating a very specific organization with few possible homonyms and low degree of ambiguity, we can rely on simple keywords. We simply exclude the quasi-homonym "國學扶輪社" (guoxue fulunshe), which referred to a publishing enterprise established in the early 20th century with no connection with the Rotary Club. Additionally, we restricted the query to the period posterior to 1919, when the first Rotary Club in China was established in Shanghai: 
```{r}
rotaryzh_doc <- search_documents_ex('"扶輪社" NOT "國學扶輪社"', corpus="shunpao", dates="[1919 TO 1947]")
head(rotaryzh_doc)
```
<br>
When we retrieved the full text of the documents (not done here), we realized that the results contained many articles in which the Rotary Club was just mentioned in passing, amidst unrelated pieces of news. Using the entire document as a text unit would only reflect the messy structure of these texts. In order to alleviate the issue, we propose to apply topic modeling on finer segments of text instead of the entire document. 

Example of problematic documents (the queried term is highlighted in red): 
```{r}
view_document("SPSP193602290401", "shunpao", query = '"扶輪社"')
```
<br>
In the above example, the length of the targeted segment is 56 characters, whereas the total length of the “article” is 9169 characters. 


## Retrieve concordance

Instead of retrieving entire documents, therefore, we will retrieve finer strings of characters using the "concordance" function included in the histtext package. This function returns the queried terms in their context. The main challenge at this stage is to define the right context size. After a careful examination of a sample of articles, we decided to set the threshold at 100 characters to minimize the risk of overlap in cases when articles contain several occurrences of the queried terms:  
```{r}
rotaryzh_conc100 <- search_concordance_ex('"扶輪社" NOT "國學扶輪社"', 
                                          context = 100, corpus="shunpao", 
                                          dates="[1919 TO 1947]")

head(rotaryzh_conc100)
```
<br>
The concordance table contains seven columns, including the unique identifier of the document (DocId), the date of publication, the title of the article (Title), the name of the periodical (Source), the queried terms (Matched), and the terms preceding (Before) and following (After) the key words. 

We can first count the number of occurrences per article: 
```{r }
rotaryzh_conc100 <- rotaryzh_conc100 %>% group_by(DocId) %>% add_tally()

rotaryzh_conc100 %>% arrange(desc(n)) 

```
<br>
Next, we create a new variable for the merged text 
```{r}
# First merge "before" and the "matched" term into a new "Text" variable
rotaryzh_conc100 <- rotaryzh_conc100 %>% mutate(Text = paste0(Before, Matched)) 
head(rotaryzh_conc100)

# Next, merge the text resulting from previous operation (Text) with "after"
rotaryzh_conc100 <- rotaryzh_conc100 %>% mutate(Text = paste0(Text, After)) 
head(rotaryzh_conc100)

```
<br>
Finally, we reunite the documents by merging all the occurrences they each contain: 
```{r}
library(data.table)

rotaryzh_100_united <- rotaryzh_conc100 %>%
  group_by(DocId, Date, Title, Source, grp = rleid(DocId)) %>% 
  summarise(Text = str_c(Text, collapse=' '), .groups = 'drop') %>%
  ungroup %>%
  select(-grp)

rotaryzh_100_united

```
<br>
We obtain 467 reshaped documents spanning from 1922 to 1947. Let's examine the distribution of documents over years: 
```{r}
# create variable for years   
rotaryzh_100_united <- rotaryzh_100_united %>%
  mutate(year = stringr::str_sub(Date,0,4)) %>% 
  mutate(year = as.numeric(year)) 

rotaryzh_100_united %>% 
  group_by(year) %>% count() %>%
  ggplot(aes(x=year, y=n)) + 
  geom_col(alpha = 0.8) + 
  labs(title = "The Rotary Club in the Shenbao",
       subtitle = "Number of articles mentioning '扶輪社'",
       x = "Year", 
       y = "Number of articles") 
```
<br>
Compute number of characters in each text
```{r}
rotaryzh_100_united <- rotaryzh_100_united %>% mutate(nchar = nchar(Text))
```
<br>
Save and exports results as a csv file: 
```{r eval=FALSE}
write.csv(rotaryzh_100_united, "rotaryzh_100_united.csv")
```

## Tokenize the text

The next step is tokenization, which consists in segmenting the Chinese text into meaningful units (tokens, which can be considered as the equivalent of words). For this purpose, we rely on the package [*jieba*](https://github.com/qinwf/jiebaR/). Although *jieba* was initially designed for tokenizing contemporary Chinese texts, it nonetheless gave satisfactory results on our corpus.

Load packages: 
```{r}
library(jiebaR)
library(jiebaRD)
```
<br>
Initialize jiebaR worker
```{r}
cutter <- worker()
```
<br>
Test the worker
```{r}
cutter <- worker()
```
<br>
Define the segmenting function
```{r}
seg_x <- function(x) {str_c(cutter[x], collapse = " ")}
```
<br>
Apply the function to each document (row of ltext)
```{r}
x.out <- sapply(rotaryzh_100_united$Text, seg_x, USE.NAMES = FALSE)
```
<br>
Attach the segmented text back to the data frame
```{r}
rotaryzh_100_united$text.seg <- x.out
```
<br> 
Inspect the first two rows of the data frame
```{r}
head(rotaryzh_100_united$text.seg, 2)
```
<br>
Count tokens and characters: 
```{r}
library(quanteda)

rotaryzh_100_united <- rotaryzh_100_united %>% 
  mutate(ntoken = ntoken(text.seg)) %>% 
  mutate(nchar = nchar(Text))
```

## Add metadata 

Finally, we incorporate the date of publication into the metadata in view of analyzing topical changes over time. We create different variables for enabling different degrees of temporal granularity. 

Create variable for years   
```{r}
rotaryzh_100_united <- rotaryzh_100_united %>%
  mutate(year = stringr::str_sub(Date,0,4)) %>% 
  mutate(year = as.numeric(year)) 
```
<br>
Create variable for decades  
```{r}
rotaryzh_100_united$decade <- paste0(substr(rotaryzh_100_united$Date, 0, 3), "0")
```
<br>
Create time period based on historian's prior knowledge on the Rotary Club and of the political context in pre-1949 China: 
```{r}
rotaryzh_100_united$period <- cut(rotaryzh_100_united$year, breaks = c(1919, 1929, 1937, 1948), 
                                label = c("1919-1929", "1930-1937", "1938-1948"), 
                                include.lowest = TRUE) 
```
<br>
Select relevant variables
```{r}
rotaryzh_conc100_corpus <- rotaryzh_100_united %>% 
  select(DocId, Source, Title, Text, 
         text.seg, nchar, ntoken, 
         Date, year, decade, period)
```
<br>
Save and export the tokenized corpus 
```{r eval=FALSE}
write.csv(rotaryzh_conc100_corpus, "rotaryzh_conc100_corpus.csv")
```


# Pre-processing

Next, we prepare the text data to make it readable by topic model algorithms. We decided to exclude a customized list of stop words, especially the queried terms used to build the corpus (扶輪社) and too common terms in this context (上海, 中國). We removed the words which contained less than 2 characters and occurred in less than 2 documents.

Load packages:
```{r}
library(stm)
library(stminsights)
```
<br>
Pre-processing
```{r}
# select metadata
meta <- rotaryzh_conc100_corpus %>% transmute(DocId, Title, Date, year, decade, period, ntoken, nchar)  

# create corpus
corpus <- stm::textProcessor(rotaryzh_conc100_corpus$text.seg,
                             metadata = meta, 
                             stem = FALSE, 
                             wordLengths = c(2, Inf), 
                             verbose = FALSE, 
                             customstopwords = c("上海", "扶輪社", "中國")) 
stm::plotRemoved(corpus$documents, lower.thresh = c(0,10, by=5)) 
out <- stm::prepDocuments(corpus$documents, 
                          corpus$vocab, 
                          corpus$meta, 
                          lower.thresh = 2) 
```
<br>
4767 of 5688 terms (5505 of 12414 tokens) were removed due to frequency. 5 Documents with no words were removed (they refer to documents in which the full text has been misplaced in the "title" field). The final corpus contains 462 documents, 921 terms and 6909 tokens.

Before we go on building the models, a sound reflex is to inspect more closely which documents were removed: 
```{r}
out$docs.removed
rotaryzh_conc100_corpus[c(165, 195, 280, 284, 430),  ]
```
<br>
Similarly, let's examine the words that were removed: 
```{r}
wordsremoved <- as_tibble(out$words.removed) 
wordsremoved
```
<br>
We notice that many of the removed words are English words inserted in the Chinese text. 

# Model building

Choosing the right number of topics *k* remains a highly debated question. There is no definite solution. Most topic modeling tools generally provide a set of metrics such as held-out likelihood, residual analysis, average exclusivity and semantic coherence, to help the researcher to determine the optimal number of topics for a given corpus. According to the authors of the [manual of the stm package](https://cran.r-project.org/web/packages/stm/stm.pdf), for small corpora ranging from a few hundred to a few thousand documents, the best number of topics should range between 5 and 50 topics ^[Margaret Roberts et al., “Stm: Estimation of the Structural Topic Model,” September 18, 2020, 64–65, https://CRAN.R-project.org/package=stm]. Ultimately, however, only the researcher’s interpretational needs can determine what is the most appropriate number of topics for a given specific research. 

In the stm R package, the *searchK* function provides a wide range of metrics to guide our choice, including held-out likelihood, residual analysis, average exclusivity and semantic coherence. Only the default properties (held-out likelihood, residuals, semantic coherence, lower bound) are displayed below: 
```{r}
set.seed(1111)
K<-seq(5,50, by=10) 
kresult <- searchK(out$documents, out$vocab, K, prevalence =~ year, data=out$meta, verbose=FALSE)
plot(kresult)
```

<br>
After several experiments, we decided build three models with 5, 10 and 20 topics, which will enable us to navigate different levels of granularity: 
```{r}

# 5-topic model
mod.5 <- stm::stm(out$documents, 
                   out$vocab, K=5, 
                   prevalence =~ year, 
                   data=out$meta, verbose = FALSE)

# 10-topic model
mod.10 <- stm::stm(out$documents, 
                   out$vocab, K=10, 
                   prevalence =~ year, 
                   data=out$meta, verbose = FALSE)

# 20-topic model
mod.20 <- stm::stm(out$documents, 
                   out$vocab, K=20, 
                   prevalence =~ year, 
                   data=out$meta, verbose = FALSE)
```
<br>
Next, we incorporate the time variables in the models to further analyze topical changes over time: 
```{r}
year5 <- stm::estimateEffect(1:5 ~ year, mod.5, meta=out$meta)
year10 <- stm::estimateEffect(1:10 ~ year, mod.10, meta=out$meta)
year20 <- stm::estimateEffect(1:20 ~ year, mod.20, meta=out$meta)
```
<br>
Finally, we save the models as an ".RData" file to save time and computational power in the future: 
```{r eval = FALSE}
save.image('rotaryzh.RData')
```

# Model evaluation 

To compare the three models, we can plot the semantic coherence of topics against their exclusivity. As the plot suggested, the higher the number of topics, the lower their semantic coherence, and the higher their exclusivity:   
```{r}

mod5df<-as.data.frame(cbind(c(1:5),exclusivity(mod.5), semanticCoherence(model=mod.5, out$documents), "SB5T"))
mod10df<-as.data.frame(cbind(c(1:10),exclusivity(mod.10), semanticCoherence(model=mod.10, out$documents), "SB10T"))
mod20df<-as.data.frame(cbind(c(1:20),exclusivity(mod.20), semanticCoherence(model=mod.20, out$documents), "SB20T"))

models<-rbind(mod5df, mod10df, mod20df)
colnames(models)<-c("Topic","Exclusivity", "SemanticCoherence", "Model")

models$Exclusivity<-as.numeric(as.character(models$Exclusivity))
models$SemanticCoherence<-as.numeric(as.character(models$SemanticCoherence))

options(repr.plot.width=7, repr.plot.height=6, repr.plot.res=100)

plotmodels <-ggplot(models, aes(SemanticCoherence, Exclusivity, color = Model))+
  geom_point(size = 2, alpha = 0.7) + 
  geom_text(aes(label=Topic), nudge_y=.04)+
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence", 
       subtitle = "Chinese-language corpus (Shenbao)")


plotmodels
```


# Model exploration

In the first step, we highly recommend using the package [stminsights](https://cran.r-project.org/web/packages/stminsights/vignettes/intro.html) to explore the models:  
```{r eval = FALSE}
library(stminsights)
run_stminsights()
```
<br>
[Stminsights](https://cran.r-project.org/web/packages/stminsights/vignettes/intro.html) is an R Shiny application which provides a set of visualizations and statistical tools for exploring the topics in one or across multiple models. While building on the stm package itself, it greatly facilitates the preliminary exploration. In the next sections, we shall provide the full code for reproducing and adjusting the outputs produced through the "stm insights" application. 

## Topic proportions θ

The package "stm" stores the document-topic proportions and the topic-word distributions in two matrices, θ (which is also referred to, somewhat confusingly, as γ) and β. We can then take a closer look at θ, which can be called directly from the model. Alternatively, it is possible and perhaps more convenient to use the built-in function "make.dt()". The latter allows to incorporate the metadata, which in our case is helpful since we aim to examine the influence of the data of publication in topic prevalence. The table below display the proportions of topics for each document, along with their metadata. 

Extract topic proportions for each model: 
```{r}

topicprop5<-make.dt(mod.5, meta)
topicprop10<-make.dt(mod.10, meta)
topicprop20<-make.dt(mod.20, meta)

```
<br>
Consulting the table might be a bit cumbersome unless we want to examine the topic proportions of a specific document. The "plot.STM" function associated with the "hist" argument helps to better visualize the estimates of document-topic proportions:
```{r}
plot.STM(mod.5, "hist")
plot.STM(mod.10, "hist")
plot.STM(mod.20, "hist")
```
<br>
Next, we can examine more closely the words that define the topics in order to better understand what each topic is really about.

## Word per topic β

In the stm package, the function "plot.STM" with argument "summary" displays the general distribution of topics (which topics are overall more common in the corpus) along with the most common words for each topic. In the example below, we set the number of desired words to 5:  
```{r}
plot.STM(mod.5,"summary", n=5)
plot.STM(mod.10, "summary", n=5)
plot.STM(mod.20, "summary", n=5)
```
<br>
Alternatively, we can plot words proportions per topics as bar plots using a tidy approach. In the example below, we focus on the 10-topic model: 
```{r}
# load packages

library(tidyverse)
library(tidytext)

td_beta10_zh <- tidytext::tidy(mod.10) 

options(repr.plot.width=7, repr.plot.height=8, repr.plot.res=100) 

td_beta10_zh %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics",
       caption = "Based on Shenbao (扶輪社 Corpus)")

```
<br>
Let's explore the top 10 words for each topic: 
```{r}
topic10zh_top_words <- td_beta10_zh %>%
  group_by(topic) %>%
  top_n(10, beta) 

topic10zh_top_words %>% arrange(topic, desc(beta))

```
## Topic labeling 

The function "labelTopics" (or sageLabels) provides a deeper insight on the popular words in each topic. In addition to words probabilities, other metrics can be computed, including the FREX words (FREX weights words by frequency and exclusivity to the topic), lift words (frequency divided by frequency in other topics), and score (similar to lift, but with log frequencies). In the example below, we set the number of words to 10: 

```{r}
labelTopics(mod.5, n=10)
labelTopics(mod.10, n=10)
labelTopics(mod.20, n=10)
```
<br>
For example, we can display the 10 FREX words for selected topics (1 and 3) in the 10-topic model: 
```{r}
plot.STM(mod.10, "labels", topics=c(1,3), label="frex", n=10, width=60)
```

## Word clouds

Word clouds provide a more intuitive way of visualizing word prevalence in topics. The example below displays the word clouds of the two "international" topics (2 and 3): 
```{r}
par(mfrow=c(1,2), mar=c(0,0,2,2))
cloud(mod.10, topic = 2, scale = c(4, 0.4))
cloud(mod.10, topic = 3, scale = c(4, 0.4))
```

## Perspective 

We can use the "perspective" argument to compare topics two by two. This function is helpful to better distinguish between topics that share many similar words. For instance, we can compare two internationally minded topics (2 and 3) in the 10-topic model: 
```{r}
par(mfrow=c(1,1))
plot(mod.10, type="perspectives", topics=c(2, 3))
```
<br>
We can also compare two topics that both deal with meetings (1 and 8): 
```{r}
par(mfrow=c(1,1))
plot(mod.10, type="perspectives", topics=c(8, 1))
```

## Quotations

Some topics may still be unclear and require that we closely look at a sample of representative documents in order to better understand how words translate into concrete sentences in the original articles. To retrieve the representative documents for a given topic, we can use the "findThoughts" function and then display the documents with the companion function "plotQuote": 

To apply this function, we first need to remove the five documents that were eliminated during the pre-processing: 
```{r}
docremoved <- rotaryzh_conc100_corpus[-c(165, 195, 280, 284, 430),  ]
```
<br>
Then, we can extract and plot the 3 most representative documents for topics 1 and 3:  
```{r}

thoughts1 <- findThoughts(mod.10,texts=docremoved$Text, topics=1, n=3)$docs[[1]]
thoughts3 <- findThoughts(mod.10,texts=docremoved$Text, topics=3, n=3)$docs[[1]]

par(mfrow=c(1,2), mar=c(0,0,2,2))
plotQuote(thoughts1, width=50, maxwidth=500, text.cex=0.5, main="Topic 1")
plotQuote(thoughts3, width=50, maxwidth=500, text.cex=0.5, main="Topic 3")

```

## Topic correlation 

Topic correlation serves to map connections between topics in a given model. The relations between the topics are based on the proportions of the words they have in common. The "stm" package provides two options for estimating topic correlations. The "simple" method simply thresholds the covariances, whereas the "huge" method uses a semi-parametric procedure. Let’s compare the two approaches: 
```{r}
corrsimple <- topicCorr(mod.20, method = "simple", verbose = FALSE)
corrhuge <- topicCorr(mod.20, method = "huge", verbose = FALSE)
par(mfrow=c(1,2), mar=c(0,0,2,2))
plot(corrsimple, main = "Simple method")
plot(corrhuge, main = "Huge method")
```
<br>
We can then use the package [gggraph](https://exts.ggplot2.tidyverse.org/ggraph.html) to better visualize topic proportions by adding weights to the links:
```{r}
# extract network 
stm_corrs <- get_network(model = mod.20,
                         method = 'simple',
                         labels = paste('Topic', 1:20),
                         cutoff = 0.001,
                         cutiso = TRUE)

# plot network with ggraph 
library(ggraph)

ggraph(stm_corrs, layout = 'fr') +
  geom_edge_link(
    aes(edge_width = weight),
    label_colour = '#fc8d62',
    edge_colour = '#377eb8') +
  geom_node_point(size = 4, colour = 'black')  +
  geom_node_label(
    aes(label = name, size = props),
    colour = 'black',  repel = TRUE, alpha = 0.85) +
  scale_size(range = c(2, 10), labels = scales::percent) +
  labs(size = 'Topic Proportion',  edge_width = 'Topic Correlation', title = "Simple method") + 
  scale_edge_width(range = c(1, 3)) +
  theme_graph()
```

## Interactive visualization

The "stm" package also includes a "LDAvis" function which produces an interactive visualization of an LDA topic model. The main graphical elements include:

  * Default topic circles - K circles, one for each topic, whose areas are set to be proportional to the proportions of the topics across the N total tokens in the corpus.
  * Red bars - represent the estimated number of times a given term was generated by a given topic.
    * Blue bars - represent the overall frequency of each term in the corpus
    * Topic-term circles - K×W circles whose areas are set to be proportional to the frequencies with which a given term is estimated to have been generated by the topics.
    
```{r eval = FALSE}
stm::toLDAvis(mod.5, doc=out$documents)
stm::toLDAvis(mod.10, doc=out$documents)
stm::toLDAvis(mod.20, doc=out$documents)
```
<br>

# Topics over time

In this section, we analyze the effect of time (date of publication) on topic prevalence in the three models. 

First, select topic proportions  
```{r}
topic5prop <- topicprop5 %>% select(c(2:6))
topic10prop <- topicprop10 %>% select(c(2:11))
topic20prop <- topicprop20 %>% select(c(2:21))
```
<br>
Compute topic proportions per year 
```{r}
topic_proportion_per_year5 <- aggregate(topic5prop, by = list(Year = rotaryzh_conc100_corpus$year), mean)
topic_proportion_per_year10 <- aggregate(topic10prop, by = list(Year = rotaryzh_conc100_corpus$year), mean)
topic_proportion_per_year20 <- aggregate(topic20prop, by = list(Year = rotaryzh_conc100_corpus$year), mean)
```
<br>
Reshape data frame
```{r}
library(reshape)
vizDataFrame5y <- melt(topic_proportion_per_year5, id.vars = "Year")
vizDataFrame10y <- melt(topic_proportion_per_year10, id.vars = "Year")
vizDataFrame20y <- melt(topic_proportion_per_year20, id.vars = "Year")
```
<br>
Plot topic proportions per year as bar plots: 
```{r}
require(pals)

# 5-topic model: 
ggplot(vizDataFrame5y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="The Rotary Club (扶輪社) in the Shenbao", 
       subtitle = "Topic proportion over time (5-topic model)")

# 10-topic model:
ggplot(vizDataFrame10y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="The Rotary Club (扶輪社) in the Shenbao", 
       subtitle = "Topic proportion over time (10-topic)")

# 20-topic model:
ggplot(vizDataFrame20y, aes(x=Year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "Topic") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  labs(title="The Rotary Club (扶輪社) in the Shenbao", 
       subtitle = "Topic proportion over time (20-topic)")

```

# Concluding remarks 

Methodologically, the contribution of this paper is threefold: 

  1. First, it has offered a simple yet efficient solution, based on concordance, to the problem of article segmentation in digitized newspapers. This preliminary method needs to be refined and adjusted to different types of texts depending on their varying length and relevance to the research question. 
  2. Second, this paper constitutes a rare instance of cross-lingual comparison involving the Chinese language during its transitional stage between classical and modern Chinese. As a low-resource language, pre-modern Chinese presents significant challenges for the application of natural language processing tools and computer-assisted text analysis. As E. Kaske has demonstrated (Kaske, 2007), the Chinese language was highly unstable during the century under study, which was a pivotal phase in the creation of a standard vernacular (baihua) national language (guoyu). In his data-driven study of the *Shenbao*, P. Magistry has shown that the Chinese language actually evolved through six main stages between 1872 and 1949. Further research should pay greater attention to the decisions made during the pre-processing phase and design strict protocols for evaluating the impact of tokenization and language variation on the resulting topics. From a multilingual perspective, future research could also benefit from more sophisticated techniques for the automatic alignment of topics across languages.  It could also investigate the differences between the various English periodicals included in the ProQuest collection, especially between the British North China Herald, the American China Weekly Review, and the Chinese-owned China Press. 
  3. Third, this research has demonstrated the value of combining different models with different k number of topics, instead of focusing on a single, definitive model. This multi-model approach is particularly appropriate when dealing with corpora of different sizes and with different structures. This multi-scalar reading of corpora enables scholars to navigate between different levels of granularity and to select in each model the topics that are the most relevant to the research question. 
  
The results of this topic modeling exercise can be used as a starting point for addressing more specific research questions. The inferred topics point to the existence of two main categories of articles that can be further investigated using adequate methods. On the one hand, topics related to meetings, organization, and philanthropy are generally rich with names of individuals, organizations, and locations. Named entity recognition (NER) and network analysis can then be utilized to automatically extract the names of these actors and further analyze their connections. On the other hand, topics related to lectures and discussions (forums), which are richer in semantic contents, lend themselves to a deeper examination of the discourses articulated by the various actors, using methods such as semantic and sentiment analysis. Finally, while the Rotary Club has served as a test case in this paper, our methodology can be expanded to investigate other public sphere institutions and more abstract concepts related to the public sphere. Furthermore, it can be transposed to similar digitized texts in English, Chinese, and possibly other languages, beyond the specific corpora utilized in this research. 

# References 

Armand, Cécile. “Foreign Clubs with Chinese Flavor: The Rotary Club of Shanghai and the Politics of Language.” In Knowledge, Power, and Networks: Elites in Transition in Modern China, edited by Cécile Armand, Christian Henriot, and Huei-min Sun, 233–59. Leiden: Brill, 2022.
Huang, Philip C.C. “‘Public Sphere “/”Civil Society’ in China? The Third Realm between State and Society.” Modern China Modern China 19, no. 2 (1993): 216–40.
Kaske, Elisabeth. The Politics of Language in Chinese Education, 1895-1919. Vol. 82. Sinica Leidensia. Leiden: Brill, 2007.
Magistry, Pierre. “Languages(s) of the Shun-Pao, a Computational Linguistics Account.” In 10th International Conference of Digital Archives and Digital Humanities. Taipei, Taiwan, 2019.
Rankin, Mary Backus. “The Origins of a Chinese Public Sphere. Local Elites and Community Affairs in the Late Imperial Period.” Études Chinoises 9, no. 2 (1990): 13–60.
Wagner, Rudolf G, ed. Joining the Global Public: Word, Image, and City in Early Chinese Newspapers, 1870-1910. Albany, NY: State University of New York Press, 2007.
Wakeman, Frederic. “The Civil Society and Public Sphere Debate: Western Reflections on Chinese Political Culture.” Modern China 19, no. 2 (1993): 108–38.

# Acknowledgements

This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 788476) and a CCFK grant.